{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics \n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing method\n",
    "def cat_2_num(df:pd.DataFrame):\n",
    "    cat_columns = df.select_dtypes(['object']).columns\n",
    "    df[cat_columns] = df[cat_columns].astype('category')\n",
    "    df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German dataset\n",
    "def German():\n",
    "    df = pd.read_table(os.path.join(\"datasets\",\"german.data-numeric.txt\"),delim_whitespace = True, header = None)\n",
    "    return train_test_split(df.drop(columns = [24]), df[24], test_size=0.3, random_state=SEED, stratify=df[24]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Australian():\n",
    "    df = pd.read_table(os.path.join(\"datasets\",\"australian.dat\"),delim_whitespace = True, header = None)\n",
    "    return train_test_split(df.drop(columns = [14]), df[14], test_size=0.3, random_state=SEED, stratify=df[14]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crx():\n",
    "    df = pd.read_csv(os.path.join(\"datasets\",\"crx.data\"), header = None)\n",
    "    # drop entries with ?\n",
    "    df = df.replace(\"?\", np.nan).dropna()\n",
    "    # convert category data to numerical data\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = [15]), df[15], test_size=0.3, random_state=SEED, stratify=df[15]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Horse():\n",
    "    df = pd.read_table(os.path.join(\"datasets\",\"horse-colic.data\"), delim_whitespace=True, header = None)\n",
    "    # instead of dropna, treat '?' as a separate class. reason: drop would leave only 68 entries\n",
    "    df = cat_2_num(df)\n",
    "\n",
    "    df_test = pd.read_table(os.path.join(\"datasets\",\"horse-colic.test\"), delim_whitespace=True, header = None)\n",
    "    df_test = cat_2_num(df_test)\n",
    "    return df.drop(columns = [24]), df_test.drop(columns = [24]), df[24], df_test[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vehicle():\n",
    "    vehicles_dfs = [pd.read_table(os.path.join(\"datasets\",f), delim_whitespace=True, header = None) for f in os.listdir('datasets') if f.startswith('xa')]\n",
    "    df = pd.concat(vehicles_dfs)\n",
    "    df.replace(\"?\", np.nan).notna()\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = [18]), df[18], test_size=0.3, random_state=SEED, stratify=df[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pumpkin():\n",
    "    df = pd.read_excel(os.path.join(\"datasets\",'Pumpkin_Seeds_Dataset.xlsx'), sheet_name='Pumpkin_Seeds_Dataset',engine='openpyxl')\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = ['Class']), df['Class'], test_size=0.3, random_state=SEED, stratify=df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Water():\n",
    "    df = pd.read_csv(os.path.join(\"datasets\",'water_potability.csv'))\n",
    "    df = df.dropna()\n",
    "    return train_test_split(df.drop(columns = ['Potability']), df['Potability'], test_size=0.3, random_state=SEED, stratify=df['Potability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Banking():\n",
    "    df = pd.read_csv(os.path.join(\"datasets\",'banking.csv'))\n",
    "    df = df.dropna()\n",
    "    df = cat_2_num(df)\n",
    "    # random sample by class ( balance out data )\n",
    "    g = df.groupby('y')\n",
    "    df = g.apply(lambda x: x.sample(1500).reset_index(drop=True))\n",
    "    return train_test_split(df.drop(columns = ['y']), df['y'], test_size=0.3, random_state=SEED, stratify=df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(\"Grid searching best parameters for decision tree\")\n",
    "    param_grid = { \n",
    "        'criterion': ['gini','entropy'],\n",
    "        'splitter': ['best','random'],\n",
    "        'max_features': ['sqrt','log2'],\n",
    "        'max_depth':list(range(1,10))\n",
    "    }\n",
    "\n",
    "    # grid search for best parameters\n",
    "    \n",
    "    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv= 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    # Create Decision Tree classifer object\n",
    "    clf = DecisionTreeClassifier(criterion = best_params['criterion'],splitter = best_params['splitter'], \n",
    "    max_features = best_params['max_features'], max_depth = best_params['max_depth'], random_state=SEED)\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy for decision tree:\", acc)\n",
    "    print(\"F-score for decision tree:\", f)\n",
    "    print(\"==============================================================\")\n",
    "    return acc, f, best_params\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ramdom forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(X_train, X_test, y_train, y_test):\n",
    "    print(\"Random searching best parameters for random forest\")\n",
    "    param_grid = {'n_estimators': list(range(800, 1600, 200)),\n",
    "               'max_depth': list(range(10,110,10)),\n",
    "               'min_samples_split': [2,5,10],\n",
    "               'min_samples_leaf': [1,2,4],\n",
    "               'bootstrap': [True, False]}\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    grid_search = RandomizedSearchCV(RandomForestClassifier(), param_grid, cv= 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators = best_params['n_estimators'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        min_samples_split = best_params['min_samples_split'], \n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        bootstrap=best_params['bootstrap'],\n",
    "        random_state = SEED)\n",
    "    # Train the model on training data\n",
    "    rf = rf.fit(X_train, y_train)\n",
    "    # Use the forest's predict method on the test data\n",
    "    y_pred = rf.predict(X_test)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy for random forest:\", acc)\n",
    "    print(\"Best F-score for random forest:\",f)\n",
    "    print(\"==============================================================\")\n",
    "    return acc, f, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    print(\"Random searching best parameters for SVM\")\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'kernel' : ['rbf', 'linear', 'poly','sigmoid']\n",
    "        }\n",
    "    \n",
    "    grid_search = RandomizedSearchCV(svm.SVC(), param_grid, cv = 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(\n",
    "        C=best_params['C'],\n",
    "        kernel=best_params['kernel'],\n",
    "        random_state = SEED) # Linear Kernel\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(X_train, y_train)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "#         print(\"kernel\", i)\n",
    "#         print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "#         print(\"F-score:\",f)\n",
    "\n",
    "    print(\"Accuracy for svm:\",acc)\n",
    "    print(\"Best F-score for svm:\",f)\n",
    "    print(\"==========================================\")\n",
    "    return acc, f, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(hp):\n",
    "    dr = 0.2 #dropout rate\n",
    "    no_neurons_1 = hp.Float('no_neurons_1', 32, 256, step=32)\n",
    "\n",
    "    rate = hp.Float('learning_rate', 0.001, 0.5, sampling=\"log\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(no_neurons_1, input_shape = (X_train.shape[1],), activation = \"relu\"),    # not sure if it is best practice\n",
    "        Dropout(rate = dr),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = keras.optimizers.SGD(learning_rate = rate),\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(n,l, X_train, X_test, y_train, y_test):\n",
    "    dr = 0.2 #dropout rate\n",
    "    early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\n",
    "    max_epochs = 100 #number of maximum epochs\n",
    "    batch = 64 #batch size\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n, input_shape = (X_train.shape[1],), activation = \"relu\"),\n",
    "        Dropout(rate = dr),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = keras.optimizers.SGD(learning_rate = l),\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]\n",
    "    )\n",
    "    model.fit(X_train, y_train,\n",
    "             validation_data=(X_test, y_test),\n",
    "             batch_size = batch,\n",
    "             epochs = max_epochs,\n",
    "             verbose = 0, \n",
    "             callbacks = [early_stopping])\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"Accuracy for neural network:\",acc)\n",
    "    print(\"Best F-score for neural network:\",f)\n",
    "    print(\"==============================================================\")\n",
    "    \n",
    "    return acc, f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(X_train, X_test, y_train, y_test):\n",
    "    tuner = kt.RandomSearch(\n",
    "        model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=10,\n",
    "        overwrite=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\n",
    "    max_epochs = 100 #number of maximum epochs\n",
    "    batch = 64 #batch size\n",
    "\n",
    "    tuner.search(X_train, y_train,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 batch_size = batch,\n",
    "                 epochs = max_epochs,\n",
    "                 verbose = 0, \n",
    "                 callbacks = [early_stopping])\n",
    "\n",
    "    n = tuner.get_best_hyperparameters()[0].get(\"no_neurons_1\")\n",
    "    l = tuner.get_best_hyperparameters()[0].get(\"learning_rate\")\n",
    "\n",
    "    return best_model(n,l, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(X_train, X_test, y_train, y_test):\n",
    "    acc_dt, f_dt, best_params_dt = DT(X_train, X_test, y_train, y_test)\n",
    "    acc_rf, f_rf, best_params_rf = RF(X_train, X_test, y_train, y_test)\n",
    "    acc_svm, f_svm, best_params_svm = SVM(X_train, X_test, y_train, y_test)\n",
    "    acc_nn, f_nn, best_params_nn = NN(X_train, X_test, y_train, y_test)\n",
    "    return {\n",
    "        'model':['decision tree', 'random forest', 'svm', 'neural network'],\n",
    "        'accuracy':[acc_dt, acc_rf, acc_svm, acc_nn],\n",
    "        'f1_score':[f_dt, f_rf, f_svm, f_nn],\n",
    "        'model_parameters':[best_params_dt, best_params_rf, best_params_svm, best_params_nn]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = Australian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    268\n",
       "1    215\n",
       "Name: 14, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()\n",
    "NN(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "10/10 [==============================] - 0s 767us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# for testing only\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m German()\n\u001b[1;32m----> 3\u001b[0m NN(X_train, X_test, y_train, y_test)\n",
      "Cell \u001b[1;32mIn [17], line 22\u001b[0m, in \u001b[0;36mNN\u001b[1;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     19\u001b[0m n \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_hyperparameters()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mno_neurons_1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m l \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mget_best_hyperparameters()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[39mreturn\u001b[39;00m best_model(n,l, X_train, X_test, y_train, y_test)\n",
      "Cell \u001b[1;32mIn [37], line 25\u001b[0m, in \u001b[0;36mbest_model\u001b[1;34m(n, l, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     17\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train,\n\u001b[0;32m     18\u001b[0m          validation_data\u001b[39m=\u001b[39m(X_test, y_test),\n\u001b[0;32m     19\u001b[0m          batch_size \u001b[39m=\u001b[39m batch,\n\u001b[0;32m     20\u001b[0m          epochs \u001b[39m=\u001b[39m max_epochs,\n\u001b[0;32m     21\u001b[0m          verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \n\u001b[0;32m     22\u001b[0m          callbacks \u001b[39m=\u001b[39m [early_stopping])\n\u001b[0;32m     24\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m---> 25\u001b[0m \u001b[39mprint\u001b[39m(pd\u001b[39m.\u001b[39;49mSeries(y_pred)\u001b[39m.\u001b[39mvalue_counts())\n\u001b[0;32m     26\u001b[0m acc \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39maccuracy_score(y_test, y_pred)\n\u001b[0;32m     27\u001b[0m f \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mf1_score(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Documents\\GitHub\\CZ4032-Project2\\venv\\lib\\site-packages\\pandas\\core\\series.py:451\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    449\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    450\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     data \u001b[39m=\u001b[39m sanitize_array(data, index, dtype, copy)\n\u001b[0;32m    453\u001b[0m     manager \u001b[39m=\u001b[39m get_option(\u001b[39m\"\u001b[39m\u001b[39mmode.data_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    454\u001b[0m     \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblock\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Documents\\GitHub\\CZ4032-Project2\\venv\\lib\\site-packages\\pandas\\core\\construction.py:601\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[0;32m    598\u001b[0m             subarr \u001b[39m=\u001b[39m cast(np\u001b[39m.\u001b[39mndarray, subarr)\n\u001b[0;32m    599\u001b[0m             subarr \u001b[39m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[1;32m--> 601\u001b[0m subarr \u001b[39m=\u001b[39m _sanitize_ndim(subarr, data, dtype, index, allow_2d\u001b[39m=\u001b[39;49mallow_2d)\n\u001b[0;32m    603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(subarr, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    604\u001b[0m     \u001b[39m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[0;32m    605\u001b[0m     dtype \u001b[39m=\u001b[39m cast(np\u001b[39m.\u001b[39mdtype, dtype)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Documents\\GitHub\\CZ4032-Project2\\venv\\lib\\site-packages\\pandas\\core\\construction.py:652\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[1;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[39mif\u001b[39;00m allow_2d:\n\u001b[0;32m    651\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m--> 652\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mData must be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    653\u001b[0m \u001b[39mif\u001b[39;00m is_object_dtype(dtype) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m    654\u001b[0m     \u001b[39m# i.e. PandasDtype(\"O\")\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     result \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39masarray_tuplesafe(data, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# for testing only\n",
    "X_train, X_test, y_train, y_test = German()\n",
    "NN(X_train, X_test, y_train, y_test)\n",
    "# all prediction value is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dataset: German\n",
      "=========== Training Set ===========\n",
      "X_train: (700, 24)\n",
      "y_train: (700,)\n",
      "value counts:\n",
      "1    490\n",
      "2    210\n",
      "Name: 24, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (300, 24)\n",
      "y_test: (300,)\n",
      "value counts:\n",
      "1    210\n",
      "2     90\n",
      "Name: 24, dtype: int64\n",
      "=========================\n",
      "Current dataset: Australian\n",
      "=========== Training Set ===========\n",
      "X_train: (483, 14)\n",
      "y_train: (483,)\n",
      "value counts:\n",
      "0    268\n",
      "1    215\n",
      "Name: 14, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (207, 14)\n",
      "y_test: (207,)\n",
      "value counts:\n",
      "0    115\n",
      "1     92\n",
      "Name: 14, dtype: int64\n",
      "=========================\n",
      "Current dataset: Crx\n",
      "=========== Training Set ===========\n",
      "X_train: (457, 15)\n",
      "y_train: (457,)\n",
      "value counts:\n",
      "1    250\n",
      "0    207\n",
      "Name: 15, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (196, 15)\n",
      "y_test: (196,)\n",
      "value counts:\n",
      "1    107\n",
      "0     89\n",
      "Name: 15, dtype: int64\n",
      "=========================\n",
      "Current dataset: Horse\n",
      "=========== Training Set ===========\n",
      "X_train: (300, 27)\n",
      "y_train: (300,)\n",
      "value counts:\n",
      "0        56\n",
      "3111     33\n",
      "3205     29\n",
      "2208     20\n",
      "2205     13\n",
      "         ..\n",
      "5205      1\n",
      "2305      1\n",
      "5000      1\n",
      "7400      1\n",
      "11300     1\n",
      "Name: 24, Length: 61, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (68, 27)\n",
      "y_test: (68,)\n",
      "value counts:\n",
      "0        11\n",
      "3111      8\n",
      "3205      6\n",
      "2209      4\n",
      "2205      4\n",
      "2208      3\n",
      "7111      3\n",
      "4124      2\n",
      "4206      2\n",
      "31110     2\n",
      "2113      2\n",
      "400       2\n",
      "3209      2\n",
      "1400      2\n",
      "3124      2\n",
      "6112      2\n",
      "6111      1\n",
      "2206      1\n",
      "1124      1\n",
      "5110      1\n",
      "2300      1\n",
      "2111      1\n",
      "5111      1\n",
      "7113      1\n",
      "8405      1\n",
      "3113      1\n",
      "2112      1\n",
      "Name: 24, dtype: int64\n",
      "=========================\n",
      "Current dataset: Vehicle\n",
      "=========== Training Set ===========\n",
      "X_train: (592, 18)\n",
      "y_train: (592,)\n",
      "value counts:\n",
      "0    153\n",
      "2    152\n",
      "1    148\n",
      "3    139\n",
      "Name: 18, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (254, 18)\n",
      "y_test: (254,)\n",
      "value counts:\n",
      "0    65\n",
      "2    65\n",
      "1    64\n",
      "3    60\n",
      "Name: 18, dtype: int64\n",
      "=========================\n",
      "Current dataset: Pumpkin\n",
      "=========== Training Set ===========\n",
      "X_train: (1750, 12)\n",
      "y_train: (1750,)\n",
      "value counts:\n",
      "0    910\n",
      "1    840\n",
      "Name: Class, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (750, 12)\n",
      "y_test: (750,)\n",
      "value counts:\n",
      "0    390\n",
      "1    360\n",
      "Name: Class, dtype: int64\n",
      "=========================\n",
      "Current dataset: Water\n",
      "=========== Training Set ===========\n",
      "X_train: (1407, 9)\n",
      "y_train: (1407,)\n",
      "value counts:\n",
      "False    840\n",
      "True     567\n",
      "Name: Potability, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (604, 9)\n",
      "y_test: (604,)\n",
      "value counts:\n",
      "False    360\n",
      "True     244\n",
      "Name: Potability, dtype: int64\n",
      "=========================\n",
      "Current dataset: Banking\n",
      "=========== Training Set ===========\n",
      "X_train: (2100, 15)\n",
      "y_train: (2100,)\n",
      "value counts:\n",
      "False    1050\n",
      "True     1050\n",
      "Name: y, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (900, 15)\n",
      "y_test: (900,)\n",
      "value counts:\n",
      "False    450\n",
      "True     450\n",
      "Name: y, dtype: int64\n",
      "=========================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=========================\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[39m# model_score = run_all_models(X_train, X_test, y_train, y_test)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[39m# model_score['dataset'] = [dataset_getter.__name__]*4\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[39m# stats_list.append(pd.DataFrame(model_score))\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m pd\u001b[39m.\u001b[39;49mconcat(stats_list)\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mmodel_scores.tsv\u001b[39m\u001b[39m\"\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Documents\\GitHub\\CZ4032-Project2\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Documents\\GitHub\\CZ4032-Project2\\venv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    348\u001b[0m         objs,\n\u001b[0;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Documents\\GitHub\\CZ4032-Project2\\venv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    401\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[0;32m    403\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "#German dataset\n",
    "dataset_funcs = [German, Australian, Crx, Horse, Vehicle, Pumpkin, Water, Banking]\n",
    "stats_list = []\n",
    "for dataset_getter in dataset_funcs:\n",
    "    print(f\"Current dataset: {dataset_getter.__name__}\")\n",
    "    X_train, X_test, y_train, y_test = dataset_getter()\n",
    "    print(\"=========== Training Set ===========\")\n",
    "    print(f\"X_train: {X_train.shape}\\ny_train: {y_train.shape}\\nvalue counts:\\n{y_train.value_counts()}\")\n",
    "    print(\"=========== Testing Set ===========\")\n",
    "    print(f\"X_test: {X_test.shape}\\ny_test: {y_test.shape}\\nvalue counts:\\n{y_test.value_counts()}\")\n",
    "    print(\"=========================\")\n",
    "    # model_score = run_all_models(X_train, X_test, y_train, y_test)\n",
    "    # model_score['dataset'] = [dataset_getter.__name__]*4\n",
    "    # stats_list.append(pd.DataFrame(model_score))\n",
    "pd.concat(stats_list).to_csv(\"model_scores.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "53ab2871ffcc1d5758548d7aac039882a39be9d4e8a177725cd1279cfa69c6fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
