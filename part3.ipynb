{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics \n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German dataset\n",
    "def German():\n",
    "    df = pd.read_table(os.path.join(\"datasets\",\"german.data-numeric.txt\"),delim_whitespace = True, header = None)\n",
    "#     display(df.head())\n",
    "    X = df.drop(columns = [24])\n",
    "    y = df[24]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED) # 70% training and 30% test\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Australian():\n",
    "    df = pd.read_table(os.path.join(\"datasets\",\"german.data-numeric.txt\"),delim_whitespace = True, header = None)\n",
    "#     display(df.head())\n",
    "    X = df.drop(columns = [24])\n",
    "    y = df[24]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED) # 70% training and 30% test\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(\"Grid searching best parameters for decision tree\")\n",
    "    param_grid = { \n",
    "        'criterion': ['gini','entropy'],\n",
    "        'splitter': ['best','random'],\n",
    "        'max_features': ['sqrt','log2'],\n",
    "        'max_depth':list(range(1,10))\n",
    "    }\n",
    "\n",
    "    # grid search for best parameters\n",
    "    \n",
    "    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv= 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    # Create Decision Tree classifer object\n",
    "    clf = DecisionTreeClassifier(criterion = best_params['criterion'],splitter = best_params['splitter'], \n",
    "    max_features = best_params['max_features'], max_depth = best_params['max_depth'], random_state=SEED)\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy for decision tree:\", acc)\n",
    "    print(\"F-score for decision tree:\", f)\n",
    "    print(\"\\n\")\n",
    "    return acc, f, best_params\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ramdom forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(X_train, X_test, y_train, y_test):\n",
    "    print(\"Grid searching best parameters for random forest\")\n",
    "    param_grid = {'n_estimators': list(range(800, 1600, 200)),\n",
    "               'max_depth': list(range(10,110,10)),\n",
    "               'min_samples_split': [2,5,10],\n",
    "               'min_samples_leaf': [1,2,4],\n",
    "               'bootstrap': [True, False]}\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    grid_search = RandomizedSearchCV(RandomForestClassifier(), param_grid, cv= 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators = best_params['n_estimators'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        min_samples_split = best_params['min_samples_split'], \n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        bootstrap=best_params['bootstrap'],\n",
    "        random_state = SEED)\n",
    "    # Train the model on training data\n",
    "    rf = rf.fit(X_train, y_train)\n",
    "    # Use the forest's predict method on the test data\n",
    "    y_pred = rf.predict(X_test)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy for random forest:\", acc)\n",
    "    print(\"Best F-score for random forest:\",f)\n",
    "    print(\"\\n\")\n",
    "    return acc, f, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    print(\"Grid searching best parameters for SVM\")\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'gamma' : [0.05, 0.1, 0.15, 0.20, 0.25], \n",
    "        'degree' : list(range(1,7)), \n",
    "        'kernel' : ['rbf', 'linear', 'poly','sigmoid']\n",
    "        }\n",
    "    \n",
    "    grid_search = RandomizedSearchCV(svm.SVC(), param_grid, cv = 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(\n",
    "        C=best_params['C'],\n",
    "        gamma=best_params['gamma'],\n",
    "        degree=best_params['degree'],\n",
    "        kernel=best_params['kernel'],\n",
    "        random_state = SEED) # Linear Kernel\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(X_train, y_train)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "#         print(\"kernel\", i)\n",
    "#         print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "#         print(\"F-score:\",f)\n",
    "\n",
    "    print(\"Accuracy for svm:\",acc)\n",
    "    print(\"Best F-score for svm:\",f)\n",
    "    print(\"\\n\")\n",
    "    return acc, f, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(hp):\n",
    "    dr = 0.2 #dropout rate\n",
    "    no_neurons_1 = hp.Float('no_neurons_1', 32, 256, step=32)\n",
    "\n",
    "    rate = hp.Float('learning_rate', 0.001, 0.5, sampling=\"log\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(no_neurons_1, input_shape = (X_train.shape[1],), activation = \"relu\"),    # not sure if it is best practice\n",
    "        Dropout(rate = dr),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = keras.optimizers.SGD(learning_rate = rate),\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(n,l, X_train, X_test, y_train, y_test):\n",
    "    dr = 0.2 #dropout rate\n",
    "    early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\n",
    "    max_epochs = 100 #number of maximum epochs\n",
    "    batch = 64 #batch size\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n, input_shape = (X_train.shape[1],), activation = \"relu\"),\n",
    "        Dropout(rate = dr),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = keras.optimizers.SGD(learning_rate = l),\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]\n",
    "    )\n",
    "    model.fit(X_train, y_train,\n",
    "             validation_data=(X_test, y_test),\n",
    "             batch_size = batch,\n",
    "             epochs = max_epochs,\n",
    "             verbose = 0, \n",
    "             callbacks = [early_stopping])\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"Accuracy for neural network:\",acc)\n",
    "    print(\"Best F-score for neural network:\",f)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return acc, f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(X_train, X_test, y_train, y_test):\n",
    "    tuner = kt.RandomSearch(\n",
    "        model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=10,\n",
    "        overwrite=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\n",
    "    max_epochs = 100 #number of maximum epochs\n",
    "    batch = 64 #batch size\n",
    "\n",
    "    tuner.search(X_train, y_train,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 batch_size = batch,\n",
    "                 epochs = max_epochs,\n",
    "                 verbose = 0, \n",
    "                 callbacks = [early_stopping])\n",
    "\n",
    "    n = tuner.get_best_hyperparameters()[0].get(\"no_neurons_1\")\n",
    "    l = tuner.get_best_hyperparameters()[0].get(\"learning_rate\")\n",
    "\n",
    "    return best_model(n,l, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(X_train, X_test, y_train, y_test):\n",
    "    acc_dt, f_dt, best_params_dt = DT(X_train, X_test, y_train, y_test)\n",
    "    acc_rf, f_rf, best_params_rf = RF(X_train, X_test, y_train, y_test)\n",
    "    acc_svm, f_svm, best_params_svm = SVM(X_train, X_test, y_train, y_test)\n",
    "    acc_nn, f_nn, best_params_nn = NN(X_train, X_test, y_train, y_test)\n",
    "    return {\n",
    "        'model':['decision tree', 'random forest', 'svm', 'neural network'],\n",
    "        'accuracy':[acc_dt, acc_rf, acc_svm, acc_nn],\n",
    "        'f1_score':[f_dt, f_rf, f_svm, f_nn],\n",
    "        'model_parameters':[best_params_dt, best_params_rf, best_params_svm, best_params_nn]\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid searching best parameters for decision tree\n",
      "Accuracy for decision tree: 0.6866666666666666\n",
      "F-score for decision tree: 0.7991452991452991\n",
      "\n",
      "\n",
      "Grid searching best parameters for random forest\n",
      "Accuracy for random forest: 0.76\n",
      "Best F-score for random forest: 0.8461538461538461\n",
      "\n",
      "\n",
      "Grid searching best parameters for SVM\n"
     ]
    }
   ],
   "source": [
    "#German dataset\n",
    "X_train, X_test, y_train, y_test = German()\n",
    "german_model_score = run_all_models(X_train, X_test, y_train, y_test)\n",
    "german_model_score['dataset'] = ['german']*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "53ab2871ffcc1d5758548d7aac039882a39be9d4e8a177725cd1279cfa69c6fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
