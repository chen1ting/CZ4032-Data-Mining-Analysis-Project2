{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics \n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing method\n",
    "def cat_2_num(df:pd.DataFrame):\n",
    "    cat_columns = df.select_dtypes(['object']).columns\n",
    "    df[cat_columns] = df[cat_columns].astype('category')\n",
    "    df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German dataset\n",
    "def German():\n",
    "    df = pd.read_table(os.path.join(\"datasets\",\"german.data-numeric.txt\"),delim_whitespace = True, header = None)\n",
    "    return train_test_split(df.drop(columns = [24]), df[24], test_size=0.3, random_state=SEED, stratify=df[24]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Australian():\n",
    "    df = pd.read_table(os.path.join(\"datasets\",\"australian.dat\"),delim_whitespace = True, header = None)\n",
    "    return train_test_split(df.drop(columns = [14]), df[14], test_size=0.3, random_state=SEED, stratify=df[14]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crx():\n",
    "    df = pd.read_csv(os.path.join(\"datasets\",\"crx.data\"), header = None)\n",
    "    # drop entries with ?\n",
    "    df = df.replace(\"?\", np.nan).dropna()\n",
    "    # convert category data to numerical data\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = [15]), df[15], test_size=0.3, random_state=SEED, stratify=df[15]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Horse():\n",
    "    df = pd.read_table(os.path.join(\"datasets\",\"horse-colic.data\"), delim_whitespace=True, header = None)\n",
    "    # instead of dropna, treat '?' as a separate class. reason: drop would leave only 68 entries\n",
    "    df = cat_2_num(df)\n",
    "\n",
    "    df_test = pd.read_table(os.path.join(\"datasets\",\"horse-colic.test\"), delim_whitespace=True, header = None)\n",
    "    df_test = cat_2_num(df_test)\n",
    "    return df.drop(columns = [24]), df_test.drop(columns = [24]), df[24], df_test[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vehicle():\n",
    "    vehicles_dfs = [pd.read_table(os.path.join(\"datasets\",f), delim_whitespace=True, header = None) for f in os.listdir('datasets') if f.startswith('xa')]\n",
    "    df = pd.concat(vehicles_dfs)\n",
    "    df.replace(\"?\", np.nan).notna()\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = [18]), df[18], test_size=0.3, random_state=SEED, stratify=df[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pumpkin():\n",
    "    df = pd.read_excel(os.path.join(\"datasets\",'Pumpkin_Seeds_Dataset.xlsx'), sheet_name='Pumpkin_Seeds_Dataset',engine='openpyxl')\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = ['Class']), df['Class'], test_size=0.3, random_state=SEED, stratify=df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Water():\n",
    "    df = pd.read_csv(os.path.join(\"datasets\",'water_potability.csv'))\n",
    "    df = df.dropna()\n",
    "    return train_test_split(df.drop(columns = ['Potability']), df['Potability'].astype(bool), test_size=0.3, random_state=SEED, stratify=df['Potability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Banking():\n",
    "    df = pd.read_csv(os.path.join(\"datasets\",'banking.csv'))\n",
    "    df = df.dropna()\n",
    "    df = cat_2_num(df)\n",
    "    # random sample by class ( balance out data )\n",
    "    g = df.groupby('y')\n",
    "    df = g.apply(lambda x: x.sample(1500).reset_index(drop=True))\n",
    "    return train_test_split(df.drop(columns = ['y']), df['y'].astype(bool), test_size=0.3, random_state=SEED, stratify=df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(\"Grid searching best parameters for decision tree\")\n",
    "    param_grid = { \n",
    "        'criterion': ['gini','entropy'],\n",
    "        'splitter': ['best','random'],\n",
    "        'max_features': ['sqrt','log2'],\n",
    "        'max_depth':list(range(1,10))\n",
    "    }\n",
    "\n",
    "    # grid search for best parameters\n",
    "    \n",
    "    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv= 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    # Create Decision Tree classifer object\n",
    "    clf = DecisionTreeClassifier(criterion = best_params['criterion'],splitter = best_params['splitter'], \n",
    "    max_features = best_params['max_features'], max_depth = best_params['max_depth'], random_state=SEED)\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy for decision tree:\", acc)\n",
    "    print(\"F-score for decision tree:\", f)\n",
    "    print(\"==============================================================\")\n",
    "    return acc, f, best_params\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ramdom forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(X_train, X_test, y_train, y_test):\n",
    "    print(\"Random searching best parameters for random forest\")\n",
    "    param_grid = {'n_estimators': list(range(800, 1600, 200)),\n",
    "               'max_depth': list(range(10,110,10)),\n",
    "               'min_samples_split': [2,5,10],\n",
    "               'min_samples_leaf': [1,2,4],\n",
    "               'bootstrap': [True, False]}\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    grid_search = RandomizedSearchCV(RandomForestClassifier(), param_grid, cv= 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators = best_params['n_estimators'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        min_samples_split = best_params['min_samples_split'], \n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        bootstrap=best_params['bootstrap'],\n",
    "        random_state = SEED)\n",
    "    # Train the model on training data\n",
    "    rf = rf.fit(X_train, y_train)\n",
    "    # Use the forest's predict method on the test data\n",
    "    y_pred = rf.predict(X_test)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy for random forest:\", acc)\n",
    "    print(\"Best F-score for random forest:\",f)\n",
    "    print(\"==============================================================\")\n",
    "    return acc, f, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    print(\"Random searching best parameters for SVM\")\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'kernel' : ['rbf', 'linear', 'poly','sigmoid']\n",
    "        }\n",
    "    \n",
    "    grid_search = RandomizedSearchCV(svm.SVC(), param_grid, cv = 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(\n",
    "        C=best_params['C'],\n",
    "        kernel=best_params['kernel'],\n",
    "        random_state = SEED) # Linear Kernel\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(X_train, y_train)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "#         print(\"kernel\", i)\n",
    "#         print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "#         print(\"F-score:\",f)\n",
    "\n",
    "    print(\"Accuracy for svm:\",acc)\n",
    "    print(\"Best F-score for svm:\",f)\n",
    "    print(\"==========================================\")\n",
    "    return acc, f, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(hp):\n",
    "    dr = 0.2 #dropout rate\n",
    "    no_neurons_1 = hp.Float('no_neurons_1', 32, 256, step=32)\n",
    "\n",
    "    rate = hp.Float('learning_rate', 0.001, 0.5, sampling=\"log\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(no_neurons_1, input_shape = (X_train.shape[1],), activation = \"relu\"),    # not sure if it is best practice\n",
    "        Dropout(rate = dr),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = keras.optimizers.SGD(learning_rate = rate),\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(n,l, X_train, X_test, y_train, y_test):\n",
    "    dr = 0.2 #dropout rate\n",
    "    early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\n",
    "    max_epochs = 100 #number of maximum epochs\n",
    "    batch = 64 #batch size\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n, input_shape = (X_train.shape[1],), activation = \"relu\"),\n",
    "        Dropout(rate = dr),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = keras.optimizers.SGD(learning_rate = l),\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]\n",
    "    )\n",
    "    model.fit(X_train, y_train,\n",
    "             validation_data=(X_test, y_test),\n",
    "             batch_size = batch,\n",
    "             epochs = max_epochs,\n",
    "             verbose = 0, \n",
    "             callbacks = [early_stopping])\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    print(\"Accuracy for neural network:\",acc)\n",
    "    print(\"Best F-score for neural network:\",f)\n",
    "    print(\"==============================================================\")\n",
    "    \n",
    "    return acc, f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(X_train, X_test, y_train, y_test):\n",
    "    tuner = kt.RandomSearch(\n",
    "        model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=10,\n",
    "        overwrite=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\n",
    "    max_epochs = 100 #number of maximum epochs\n",
    "    batch = 64 #batch size\n",
    "\n",
    "    tuner.search(X_train, y_train,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 batch_size = batch,\n",
    "                 epochs = max_epochs,\n",
    "                 verbose = 0, \n",
    "                 callbacks = [early_stopping])\n",
    "\n",
    "    n = tuner.get_best_hyperparameters()[0].get(\"no_neurons_1\")\n",
    "    l = tuner.get_best_hyperparameters()[0].get(\"learning_rate\")\n",
    "\n",
    "    return best_model(n,l, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(X_train, X_test, y_train, y_test):\n",
    "    acc_dt, f_dt, best_params_dt = DT(X_train, X_test, y_train, y_test)\n",
    "    acc_rf, f_rf, best_params_rf = RF(X_train, X_test, y_train, y_test)\n",
    "    acc_svm, f_svm, best_params_svm = SVM(X_train, X_test, y_train, y_test)\n",
    "    acc_nn, f_nn, best_params_nn = NN(X_train, X_test, y_train, y_test)\n",
    "    return {\n",
    "        'model':['decision tree', 'random forest', 'svm', 'neural network'],\n",
    "        'accuracy':[acc_dt, acc_rf, acc_svm, acc_nn],\n",
    "        'f1_score':[f_dt, f_rf, f_svm, f_nn],\n",
    "        'model_parameters':[best_params_dt, best_params_rf, best_params_svm, best_params_nn]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dataset: German\n",
      "=========== Training Set ===========\n",
      "X_train: (700, 24)\n",
      "y_train: (700,)\n",
      "value counts:\n",
      "1    490\n",
      "2    210\n",
      "Name: 24, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (300, 24)\n",
      "y_test: (300,)\n",
      "value counts:\n",
      "1    210\n",
      "2     90\n",
      "Name: 24, dtype: int64\n",
      "=========================\n",
      "Grid searching best parameters for decision tree\n",
      "Accuracy for decision tree: 0.74\n",
      "F-score for decision tree: 0.8124999999999999\n",
      "==============================================================\n",
      "Random searching best parameters for random forest\n",
      "Accuracy for random forest: 0.7766666666666666\n",
      "Best F-score for random forest: 0.8533916849015317\n",
      "==============================================================\n",
      "Random searching best parameters for SVM\n",
      "Accuracy for svm: 0.7633333333333333\n",
      "Best F-score for svm: 0.8397291196388261\n",
      "==========================================\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "Accuracy for neural network: 0.7\n",
      "Best F-score for neural network: 0.8235294117647058\n",
      "==============================================================\n",
      "Current dataset: Australian\n",
      "=========== Training Set ===========\n",
      "X_train: (483, 14)\n",
      "y_train: (483,)\n",
      "value counts:\n",
      "0    268\n",
      "1    215\n",
      "Name: 14, dtype: int64\n",
      "=========== Testing Set ===========\n",
      "X_test: (207, 14)\n",
      "y_test: (207,)\n",
      "value counts:\n",
      "0    115\n",
      "1     92\n",
      "Name: 14, dtype: int64\n",
      "=========================\n",
      "Grid searching best parameters for decision tree\n",
      "Accuracy for decision tree: 0.821256038647343\n",
      "F-score for decision tree: 0.8229665071770336\n",
      "==============================================================\n",
      "Random searching best parameters for random forest\n",
      "Accuracy for random forest: 0.8599033816425121\n",
      "Best F-score for random forest: 0.8512820512820513\n",
      "==============================================================\n",
      "Random searching best parameters for SVM\n"
     ]
    }
   ],
   "source": [
    "#German dataset\n",
    "dataset_funcs = [German, Australian, Crx, Horse, Vehicle, Pumpkin, Water, Banking]\n",
    "stats_list = []\n",
    "for dataset_getter in dataset_funcs:\n",
    "    print(f\"Current dataset: {dataset_getter.__name__}\")\n",
    "    X_train, X_test, y_train, y_test = dataset_getter()\n",
    "    print(\"=========== Training Set ===========\")\n",
    "    print(f\"X_train: {X_train.shape}\\ny_train: {y_train.shape}\\nvalue counts:\\n{y_train.value_counts()}\")\n",
    "    print(\"=========== Testing Set ===========\")\n",
    "    print(f\"X_test: {X_test.shape}\\ny_test: {y_test.shape}\\nvalue counts:\\n{y_test.value_counts()}\")\n",
    "    print(\"=========================\")\n",
    "    model_score = run_all_models(X_train, X_test, y_train, y_test)\n",
    "    model_score['dataset'] = [dataset_getter.__name__]*4\n",
    "    stats_list.append(pd.DataFrame(model_score))\n",
    "pd.concat(stats_list).to_csv(\"model_scores.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b181fbedbc45c428172ad2d3d921732af5f9721e7dc8d85197bc3988ec89914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
