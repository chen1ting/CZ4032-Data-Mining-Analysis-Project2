{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Turn interactive plotting off, show plot only when plt.show() is called\n",
    "plt.ioff()\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "INPUT = \"datasets\"\n",
    "OUTPUT = \"results\"\n",
    "if not os.path.exists(OUTPUT):\n",
    "    os.makedirs(OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing method\n",
    "def cat_2_num(df:pd.DataFrame):\n",
    "    cat_columns = df.select_dtypes(['object']).columns\n",
    "    df[cat_columns] = df[cat_columns].astype('category')\n",
    "    df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German dataset\n",
    "def German():\n",
    "    df = pd.read_table(os.path.join(INPUT,\"german.data-numeric\"),delim_whitespace = True, header = None)\n",
    "    df[24] = df[24]-1    # change label from 1,2 to 0,1\n",
    "    return train_test_split(df.drop(columns = [24]), df[24].astype(bool), test_size=0.3, random_state=SEED, stratify=df[24]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Australian():\n",
    "    df = pd.read_table(os.path.join(INPUT,\"australian.dat\"),delim_whitespace = True, header = None)\n",
    "    return train_test_split(df.drop(columns = [14]), df[14].astype(bool), test_size=0.3, random_state=SEED, stratify=df[14]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crx():\n",
    "    df = pd.read_csv(os.path.join(INPUT,\"crx.data\"), header = None)\n",
    "    # drop entries with ?\n",
    "    df = df.replace(\"?\", np.nan).dropna()\n",
    "    # convert category data to numerical data\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = [15]), df[15].astype(bool), test_size=0.3, random_state=SEED, stratify=df[15]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hepatitis():\n",
    "    df = pd.read_csv(os.path.join(INPUT,\"hepatitis.data\"), header = None)\n",
    "    df = cat_2_num(df)\n",
    "    df[19] = df[19]-1 # change to 0 or 1\n",
    "    return train_test_split(df.drop(columns = [19]), df[19].astype(bool), test_size=0.3, random_state=SEED, stratify=df[19]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ionosphere():\n",
    "    df = pd.read_csv(os.path.join(INPUT, \"ionosphere.data\"), header=None)\n",
    "    df = cat_2_num(df)\n",
    "    \n",
    "    return train_test_split(df.drop(columns = [34]), df[34].astype(bool), test_size=0.3, random_state=SEED, stratify=df[34]) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pumpkin():\n",
    "    df = pd.read_excel(os.path.join(\"datasets\",'Pumpkin_Seeds_Dataset.xlsx'), sheet_name='Pumpkin_Seeds_Dataset',engine='openpyxl')\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = ['Class']), df['Class'].astype(bool), test_size=0.3, random_state=SEED, stratify=df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5644 samples, relatively large dataset\n",
    "def Mushroom():\n",
    "    df = pd.read_csv(os.path.join(INPUT,'mushrooms.csv'))\n",
    "    df = df.replace(\"?\", np.nan).dropna()\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = ['class']), df['class'].astype(bool), test_size=0.3, random_state=SEED, stratify=df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diabetes():\n",
    "    df = pd.read_csv(os.path.join(INPUT,'diabetes_data.csv'), sep=';')\n",
    "    df = cat_2_num(df)\n",
    "    return train_test_split(df.drop(columns = ['class']), df['class'].astype(bool), test_size=0.3, random_state=SEED, stratify=df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_stats(X_train, X_test, y_train, y_test, name:str):\n",
    "    print(f\"Current dataset: {name}\")\n",
    "    print(f\"X_train: {X_train.shape}; y_train: {y_train.shape}\")#\\ny_train value counts:\\n{y_train.value_counts()}\")\n",
    "    print(f\"X_test: {X_test.shape}; y_test: {y_test.shape}\")#\\ny_test value counts:\\n{y_test.value_counts()}\")\n",
    "    print(\"\")\n",
    "    return {\n",
    "        'X_train shape':X_train.shape, \n",
    "        'y_train shape':y_train.shape, \n",
    "        'X_test shape':X_test.shape, \n",
    "        'y_test shape':y_test.shape\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "def eval_result(y_test, y_pred, model_name):\n",
    "    global counter\n",
    "    image_name=f\"{counter}_{model_name}.png\"\n",
    "    f = metrics.f1_score(y_test, y_pred)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    ctx = metrics.confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(counter)\n",
    "    sns.heatmap(ctx, cmap='Oranges', annot=True, fmt='g')\n",
    "    plt.savefig(os.path.join(OUTPUT, image_name))\n",
    "    plt.close(counter)\n",
    "    print(f\"Accuracy for {model_name}:\", acc)\n",
    "    print(f\"F-score for {model_name}:\", f)\n",
    "    print()\n",
    "    counter += 1\n",
    "    return f, acc, image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    print(f\"---------------- Decision Tree ---------------------\")\n",
    "    param_grid = { \n",
    "        'criterion': ['gini','entropy'],\n",
    "        'splitter': ['best','random'],\n",
    "        'max_features': ['sqrt','log2'],\n",
    "        'max_depth':list(range(1,10))\n",
    "    }\n",
    "\n",
    "    # grid search for best parameters\n",
    "    \n",
    "    grid_search = GridSearchCV(DecisionTreeClassifier(random_state=SEED), param_grid, cv=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    # Create Decision Tree classifer object\n",
    "    clf = DecisionTreeClassifier(\n",
    "        criterion = best_params['criterion'],\n",
    "        splitter = best_params['splitter'], \n",
    "        max_features = best_params['max_features'], \n",
    "        max_depth = best_params['max_depth'], \n",
    "        random_state=SEED)\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return eval_result(y_test, y_pred, \"decision tree\"), best_params\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ramdom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    print(f\"---------------- Random Forest ---------------------\")\n",
    "\n",
    "    print(\"Randomized searching best parameters...\")\n",
    "    param_grid = {'n_estimators': list(range(800, 1600, 200)),\n",
    "               'max_depth': list(range(10,110,10)),\n",
    "               'min_samples_split': [2,5,10],\n",
    "               'min_samples_leaf': [1,2,4],\n",
    "               'bootstrap': [True, False]}\n",
    "    grid_search = RandomizedSearchCV(RandomForestClassifier(random_state=SEED), param_grid, cv= 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best parameters found {best_params}\")\n",
    "\n",
    "    # train model using the best parameters\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators = best_params['n_estimators'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        min_samples_split = best_params['min_samples_split'], \n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        bootstrap=best_params['bootstrap'],\n",
    "        random_state = SEED)\n",
    "    # Train the model on training data\n",
    "    rf = rf.fit(X_train, y_train)\n",
    "    # Use the forest's predict method on the test data\n",
    "    y_pred = rf.predict(X_test)\n",
    "    return eval_result(y_test, y_pred, \"random forest\"), best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    print(f\"-------------------- SVM -------------------------\")\n",
    "    print(\"Randomized searching best parameters...\")\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'kernel' : ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "        }\n",
    "    \n",
    "    grid_search = RandomizedSearchCV(svm.SVC(random_state=SEED), param_grid, cv = 3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best parameters found {best_params}\")\n",
    "\n",
    "    # Create a svm Classifier\n",
    "    clf = svm.SVC(\n",
    "        C=best_params['C'],\n",
    "        kernel=best_params['kernel'],\n",
    "        random_state = SEED) # Linear Kernel\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(X_train, y_train)\n",
    "    #Predict the response for test dataset\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return eval_result(y_test, y_pred, \"svm\"), best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(hp):\n",
    "    dr = 0.2 #dropout rate\n",
    "    no_neurons_1 = hp.Float('no_neurons_1', 32, 256, step=32)\n",
    "\n",
    "    rate = hp.Float('learning_rate', 0.001, 0.5, sampling=\"log\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(no_neurons_1, input_shape = (X_train.shape[1],), activation = \"relu\"),    # not sure if it is best practice\n",
    "        Dropout(rate = dr),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = keras.optimizers.SGD(learning_rate = rate),\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(n,l, X_train, X_test, y_train, y_test):\n",
    "    dr = 0.2 #dropout rate\n",
    "    early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\n",
    "    max_epochs = 100 #number of maximum epochs\n",
    "    batch = 64 #batch size\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(n, input_shape = (X_train.shape[1],), activation = \"relu\"),\n",
    "        Dropout(rate = dr),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = keras.optimizers.SGD(learning_rate = l),\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"]\n",
    "    )\n",
    "    model.fit(X_train, y_train,\n",
    "             validation_data=(X_test, y_test),\n",
    "             batch_size = batch,\n",
    "             epochs = max_epochs,\n",
    "             verbose = 0, \n",
    "             callbacks = [early_stopping])\n",
    "    \n",
    "    y_pred = np.round(model.predict(X_test))\n",
    "    \n",
    "    return eval_result(y_test, y_pred, \"neural network\"), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(X_train, X_test, y_train, y_test):\n",
    "    tuner = kt.RandomSearch(\n",
    "        model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_trials=10,\n",
    "        overwrite=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor = \"val_loss\", patience = 3)\n",
    "    max_epochs = 100 #number of maximum epochs\n",
    "    batch = 64 #batch size\n",
    "\n",
    "    tuner.search(X_train, y_train,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 batch_size = batch,\n",
    "                 epochs = max_epochs,\n",
    "                 verbose = 0, \n",
    "                 callbacks = [early_stopping])\n",
    "\n",
    "    n = tuner.get_best_hyperparameters()[0].get(\"no_neurons_1\")\n",
    "    l = tuner.get_best_hyperparameters()[0].get(\"learning_rate\")\n",
    "\n",
    "    return best_model(n,l, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [DT, RF, SVM, NN]\n",
    "def run_all_models(X_train, X_test, y_train, y_test):\n",
    "    stats = {'model':[],'accuracy':[],'f1_score':[],'cfmtx_img':[],'model_parameters':[]}\n",
    "    for model in models:\n",
    "        (acc, f, ctx), best_params = model(X_train, X_test, y_train, y_test)\n",
    "        stats['model'].append(model.__name__)\n",
    "        stats['accuracy'].append(acc)\n",
    "        stats['f1_score'].append(f)\n",
    "        stats['cfmtx_img'].append(ctx)\n",
    "        stats['model_parameters'].append(best_params)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes only, need to delete\n",
    "# X_train, X_test, y_train, y_test = Pumpkin()\n",
    "# stats = print_dataset_stats(X_train, X_test, y_train, y_test, Mushroom.__name__)\n",
    "# model_score = run_all_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dataset: German\n",
      "X_train: (700, 24); y_train: (700,)\n",
      "X_test: (300, 24); y_test: (300,)\n",
      "\n",
      "---------------- Decision Tree ---------------------\n",
      "Accuracy for decision tree: 0.74\n",
      "F-score for decision tree: 0.5760869565217391\n",
      "\n",
      "---------------- Random Forest ---------------------\n",
      "Randomized searching best parameters...\n"
     ]
    }
   ],
   "source": [
    "#German dataset\n",
    "dataset_funcs = [German, Australian, Crx, Hepatitis, Ionosphere, Pumpkin, Mushroom, Diabetes]\n",
    "stats_list = []\n",
    "for dataset_getter in dataset_funcs:\n",
    "    X_train, X_test, y_train, y_test = dataset_getter()\n",
    "    stats = print_dataset_stats(X_train, X_test, y_train, y_test, dataset_getter.__name__)\n",
    "    model_score = run_all_models(X_train, X_test, y_train, y_test)\n",
    "    model_score['name'] = [dataset_getter.__name__]*4\n",
    "    model_score['stats'] = [stats]*4\n",
    "    stats_list.append(pd.DataFrame(model_score))\n",
    "    print(\"===============================================\\n\")\n",
    "pd.concat(stats_list).to_csv(os.path.join(OUTPUT, \"model_scores.tsv\"), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb1816ef6abe37709700f59d296504afbde16240c11088d3d5ef633232cba45c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
