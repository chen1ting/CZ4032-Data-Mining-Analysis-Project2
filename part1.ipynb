{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from functools import cmp_to_key\n",
    "import sys\n",
    "\n",
    "INPUT = \"datasets\"\n",
    "OUTPUT = \"part1_results\"\n",
    "if not os.path.exists(OUTPUT):\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing method\n",
    "\n",
    "# Change categorical data to numerical data\n",
    "def cat_2_num(df:pd.DataFrame):\n",
    "    cat_columns = df.select_dtypes(['object']).columns\n",
    "    df[cat_columns] = df[cat_columns].astype('category')\n",
    "    df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    return df\n",
    "\n",
    "# Use KMeans to get discretized data\n",
    "def get_discretization_data(df:pd.DataFrame):\n",
    "    for col_name in df.columns:\n",
    "        if(len(pd.unique(df[col_name])) <= 5):\n",
    "            return df\n",
    "        k = 5\n",
    "        k_model = KMeans(n_clusters=k)\n",
    "        k_model.fit(df[col_name].values.reshape(len(df[col_name]), 1))\n",
    "        c = pd.DataFrame(k_model.cluster_centers_, columns = list(\"a\")).sort_values(by = \"a\")\n",
    "        w = c.rolling(2).mean().iloc[1:]\n",
    "        w = np.asarray(w.values)\n",
    "        w = [i[0] for i in w]\n",
    "        w = [0] + w + [df[col_name].max()]\n",
    "        df[col_name] = pd.cut(df[col_name], w, labels = range(k))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German dataset\n",
    "def German():\n",
    "    df = pd.read_table(os.path.join(INPUT,\"german.data-numeric\"),delim_whitespace = True, header = None)\n",
    "    df = get_discretization_data(df)\n",
    "    df[24] = df[24]-1    # change label from 1,2 to 0,1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Australian():\n",
    "    df = pd.read_table(os.path.join(INPUT,\"australian.dat\"),delim_whitespace = True, header = None)\n",
    "    df = get_discretization_data(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crx():\n",
    "    df = pd.read_csv(os.path.join(INPUT,\"crx.data\"), header = None)\n",
    "    # drop entries with ?\n",
    "    df = df.replace(\"?\", np.nan).dropna()\n",
    "    # convert category data to numerical data\n",
    "    df = cat_2_num(df)\n",
    "    df = get_discretization_data(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hepatitis():\n",
    "    df = pd.read_csv(os.path.join(INPUT,\"hepatitis.data\"), header = None)\n",
    "    df = cat_2_num(df)\n",
    "    df = get_discretization_data(df)\n",
    "    df[19] = df[19]-1 # change to 0 or 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ionosphere():\n",
    "    df = pd.read_csv(os.path.join(INPUT, \"ionosphere.data\"), header=None)\n",
    "    df = cat_2_num(df)\n",
    "    df = get_discretization_data(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pumpkin():\n",
    "    df = pd.read_excel(os.path.join(\"datasets\",'Pumpkin_Seeds_Dataset.xlsx'), sheet_name='Pumpkin_Seeds_Dataset',engine='openpyxl')\n",
    "    df = cat_2_num(df)\n",
    "    df = get_discretization_data(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5644 samples, relatively large dataset\n",
    "def Mushroom():\n",
    "    df = pd.read_csv(os.path.join(INPUT,'mushrooms.csv'))\n",
    "    df = df.replace(\"?\", np.nan).dropna()\n",
    "    df = cat_2_num(df)\n",
    "    df = get_discretization_data(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diabetes():\n",
    "    df = pd.read_csv(os.path.join(INPUT,'diabetes_data.csv'), sep=';')\n",
    "    df = cat_2_num(df)\n",
    "    df = get_discretization_data(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleItem:\n",
    "    \"\"\"\n",
    "    cond_set: a dict with following fashion:\n",
    "            {item name: value, item name: value, ...}\n",
    "        e.g.\n",
    "            {A: 1, B: 1} (A, B are name of columns, here called \"item\", and in our code should be numerical index\n",
    "                          but not string)\n",
    "    class_label: just to identify the class it belongs to.\n",
    "    dataset: a list returned by read method. (see read.py)\n",
    "    cond_sup_count, rule_sup_count, support and confidence are number.\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_set, class_label, dataset):\n",
    "        self.cond_set = cond_set\n",
    "        self.class_label = class_label\n",
    "        self.cond_sup_count, self.rule_sup_count = self._get_sup_count(dataset)\n",
    "        self.support = self._get_support(len(dataset))\n",
    "        self.confidence = self._get_confidence()\n",
    "\n",
    "    # calculate condsupCount and rulesupCount\n",
    "    def _get_sup_count(self, dataset):\n",
    "        cond_sup_count = 0\n",
    "        rule_sup_count = 0\n",
    "        for case in dataset:\n",
    "            is_contained = True\n",
    "            for index in self.cond_set:\n",
    "                if self.cond_set[index] != case[index]:\n",
    "                    is_contained = False\n",
    "                    break\n",
    "            if is_contained:\n",
    "                cond_sup_count += 1\n",
    "                if self.class_label == case[-1]:\n",
    "                    rule_sup_count += 1\n",
    "        return cond_sup_count, rule_sup_count\n",
    "\n",
    "    # calculate support count\n",
    "    def _get_support(self, dataset_size):\n",
    "        return self.rule_sup_count / dataset_size\n",
    "\n",
    "    # calculate confidence\n",
    "    def _get_confidence(self):\n",
    "        if self.cond_sup_count != 0:\n",
    "            return self.rule_sup_count / self.cond_sup_count\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # print out the ruleitem\n",
    "    def print(self):\n",
    "        cond_set_output = ''\n",
    "        for item in self.cond_set:\n",
    "            cond_set_output += '(' + str(item) + ', ' + str(self.cond_set[item]) + '), '\n",
    "        cond_set_output = cond_set_output[:-2]\n",
    "        print('<({' + cond_set_output + '}, ' + str(self.cond_sup_count) + '), (' +\n",
    "              '(class, ' + str(self.class_label) + '), ' + str(self.rule_sup_count) + ')>')\n",
    "\n",
    "    # print out rule\n",
    "    def print_rule(self):\n",
    "        cond_set_output = ''\n",
    "        for item in self.cond_set:\n",
    "            cond_set_output += '(' + str(item) + ', ' + str(self.cond_set[item]) + '), '\n",
    "        cond_set_output = '{' + cond_set_output[:-2] + '}'\n",
    "        print(cond_set_output + ' -> (class, ' + str(self.class_label) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequentRuleitems:\n",
    "    \"\"\"\n",
    "    A set of frequent k-ruleitems, just using set.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.frequent_ruleitems_set = set()\n",
    "\n",
    "    # get size of set\n",
    "    def get_size(self):\n",
    "        return len(self.frequent_ruleitems_set)\n",
    "\n",
    "    # add a new ruleitem into set\n",
    "    def add(self, rule_item):\n",
    "        is_existed = False\n",
    "        for item in self.frequent_ruleitems_set:\n",
    "            if item.class_label == rule_item.class_label:\n",
    "                if item.cond_set == rule_item.cond_set:\n",
    "                    is_existed = True\n",
    "                    break\n",
    "        if not is_existed:\n",
    "            self.frequent_ruleitems_set.add(rule_item)\n",
    "\n",
    "    # append set of ruleitems\n",
    "    def append(self, sets):\n",
    "        for item in sets.frequent_ruleitems:\n",
    "            self.add(item)\n",
    "\n",
    "    # print out all frequent ruleitems\n",
    "    def print(self):\n",
    "        for item in self.frequent_ruleitems_set:\n",
    "            item.print()\n",
    "\n",
    "\n",
    "class Car:\n",
    "    \"\"\"\n",
    "    Class Association Rules (Car). If some ruleitems has the same condset, the ruleitem with the highest confidence is\n",
    "    chosen as the Possible Rule (PR). If there're more than one ruleitem with the same highest confidence, we randomly\n",
    "    select one ruleitem.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.rules = set()\n",
    "        self.pruned_rules = set()\n",
    "\n",
    "    # print out all rules\n",
    "    def print_rule(self):\n",
    "        for item in self.rules:\n",
    "            item.print_rule()\n",
    "\n",
    "    # print out all pruned rules\n",
    "    def print_pruned_rule(self):\n",
    "        for item in self.pruned_rules:\n",
    "            item.print_rule()\n",
    "\n",
    "    # add a new rule (frequent & accurate), save the ruleitem with the highest confidence when having the same condset\n",
    "    def _add(self, rule_item, minsup, minconf):\n",
    "        if rule_item.support >= minsup and rule_item.confidence >= minconf:\n",
    "            if rule_item in self.rules:\n",
    "                return\n",
    "            for item in self.rules:\n",
    "                if item.cond_set == rule_item.cond_set and item.confidence < rule_item.confidence:\n",
    "                    self.rules.remove(item)\n",
    "                    self.rules.add(rule_item)\n",
    "                    return\n",
    "                elif item.cond_set == rule_item.cond_set and item.confidence >= rule_item.confidence:\n",
    "                    return\n",
    "            self.rules.add(rule_item)\n",
    "\n",
    "    # convert frequent ruleitems into car\n",
    "    def gen_rules(self, frequent_ruleitems, minsup, minconf):\n",
    "        for item in frequent_ruleitems.frequent_ruleitems_set:\n",
    "            self._add(item, minsup, minconf)\n",
    "\n",
    "    # prune rules\n",
    "    def prune_rules(self, dataset):\n",
    "        for rule in self.rules:\n",
    "            pruned_rule = prune(rule, dataset)\n",
    "\n",
    "            is_existed = False\n",
    "            for rule in self.pruned_rules:\n",
    "                if rule.class_label == pruned_rule.class_label:\n",
    "                    if rule.cond_set == pruned_rule.cond_set:\n",
    "                        is_existed = True\n",
    "                        break\n",
    "\n",
    "            if not is_existed:\n",
    "                self.pruned_rules.add(pruned_rule)\n",
    "\n",
    "    # union new car into rules list\n",
    "    def append(self, car, minsup, minconf):\n",
    "        for item in car.rules:\n",
    "            self._add(item, minsup, minconf)\n",
    "\n",
    "\n",
    "# try to prune rule\n",
    "def prune(rule, dataset):\n",
    "    import sys\n",
    "    min_rule_error = sys.maxsize\n",
    "    pruned_rule = rule\n",
    "\n",
    "    # prune rule recursively\n",
    "    def find_prune_rule(this_rule):\n",
    "        nonlocal min_rule_error\n",
    "        nonlocal pruned_rule\n",
    "\n",
    "        # calculate how many errors the rule r make in the dataset\n",
    "        def errors_of_rule(r):\n",
    "\n",
    "            errors_number = 0\n",
    "            for case in dataset:\n",
    "                if is_satisfy(case, r) == False:\n",
    "                    errors_number += 1\n",
    "            return errors_number\n",
    "\n",
    "        rule_error = errors_of_rule(this_rule)\n",
    "        if rule_error < min_rule_error:\n",
    "            min_rule_error = rule_error\n",
    "            pruned_rule = this_rule\n",
    "        this_rule_cond_set = list(this_rule.cond_set)\n",
    "        if len(this_rule_cond_set) >= 2:\n",
    "            for attribute in this_rule_cond_set:\n",
    "                temp_cond_set = dict(this_rule.cond_set)\n",
    "                temp_cond_set.pop(attribute)\n",
    "                temp_rule = RuleItem(temp_cond_set, this_rule.class_label, dataset)\n",
    "                temp_rule_error = errors_of_rule(temp_rule)\n",
    "                if temp_rule_error <= min_rule_error:\n",
    "                    min_rule_error = temp_rule_error\n",
    "                    pruned_rule = temp_rule\n",
    "                    if len(temp_cond_set) >= 2:\n",
    "                        find_prune_rule(temp_rule)\n",
    "\n",
    "    find_prune_rule(rule)\n",
    "    return pruned_rule\n",
    "\n",
    "\n",
    "# invoked by candidate_gen, join two items to generate candidate\n",
    "def join(item1, item2, dataset):\n",
    "    if item1.class_label != item2.class_label:\n",
    "        return None\n",
    "    category1 = set(item1.cond_set)\n",
    "    category2 = set(item2.cond_set)\n",
    "    if category1 == category2:\n",
    "        return None\n",
    "    intersect = category1 & category2\n",
    "    for item in intersect:\n",
    "        if item1.cond_set[item] != item2.cond_set[item]:\n",
    "            return None\n",
    "    category = category1 | category2\n",
    "    new_cond_set = dict()\n",
    "    for item in category:\n",
    "        if item in category1:\n",
    "            new_cond_set[item] = item1.cond_set[item]\n",
    "        else:\n",
    "            new_cond_set[item] = item2.cond_set[item]\n",
    "    new_ruleitem = RuleItem(new_cond_set, item1.class_label, dataset)\n",
    "    return new_ruleitem\n",
    "\n",
    "\n",
    "# similar to Apriori-gen in algorithm Apriori\n",
    "def candidate_gen(frequent_ruleitems, dataset):\n",
    "    returned_frequent_ruleitems = FrequentRuleitems()\n",
    "    for item1 in frequent_ruleitems.frequent_ruleitems_set:\n",
    "        for item2 in frequent_ruleitems.frequent_ruleitems_set:\n",
    "            new_ruleitem = join(item1, item2, dataset)\n",
    "            if new_ruleitem:\n",
    "                returned_frequent_ruleitems.add(new_ruleitem)\n",
    "                if returned_frequent_ruleitems.get_size() >= 1000:      # not allow to store more than 1000 ruleitems\n",
    "                    return returned_frequent_ruleitems\n",
    "    return returned_frequent_ruleitems\n",
    "\n",
    "\n",
    "# main method, implementation of CBA-RG algorithm\n",
    "def rule_generator(dataset, minsup, minconf):\n",
    "    frequent_ruleitems = FrequentRuleitems()\n",
    "    car = Car()\n",
    "\n",
    "    # get large 1-ruleitems and generate rules\n",
    "    class_label = set([x[-1] for x in dataset])\n",
    "    for column in range(0, len(dataset[0])-1):\n",
    "        distinct_value = set([x[column] for x in dataset])\n",
    "        for value in distinct_value:\n",
    "            cond_set = {column: value}\n",
    "            for classes in class_label:\n",
    "                rule_item = RuleItem(cond_set, classes, dataset)\n",
    "                if rule_item.support >= minsup:\n",
    "                    frequent_ruleitems.add(rule_item)\n",
    "    car.gen_rules(frequent_ruleitems, minsup, minconf)\n",
    "    cars = car\n",
    "\n",
    "    last_cars_number = 0\n",
    "    current_cars_number = len(cars.rules)\n",
    "    while frequent_ruleitems.get_size() > 0 and current_cars_number <= 2000 and \\\n",
    "                    (current_cars_number - last_cars_number) >= 10:\n",
    "        candidate = candidate_gen(frequent_ruleitems, dataset)\n",
    "        frequent_ruleitems = FrequentRuleitems()\n",
    "        car = Car()\n",
    "        for item in candidate.frequent_ruleitems_set:\n",
    "            if item.support >= minsup:\n",
    "                frequent_ruleitems.add(item)\n",
    "        car.gen_rules(frequent_ruleitems, minsup, minconf)\n",
    "        cars.append(car, minsup, minconf)\n",
    "        last_cars_number = current_cars_number\n",
    "        current_cars_number = len(cars.rules)\n",
    "\n",
    "    return cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAR:\n",
    "    def __init__(self):\n",
    "        self.rules = set()\n",
    "        self.pruned_rules = set()\n",
    "\n",
    "    def print_rule(self):\n",
    "        for rule in self.rules:\n",
    "            rule.print()\n",
    "    \n",
    "    def print_pruned_rules(self):\n",
    "        for rule in self.pruned_rules:\n",
    "            rule.print()\n",
    "\n",
    "    def _add(self, new_rule, minsup, minconf):\n",
    "        if new_rule.support >= minsup and new_rule.confidence >= minconf:\n",
    "            if new_rule in self.rules:\n",
    "                return\n",
    "            for rule in self.rules:\n",
    "                if rule.cond_set == new_rule.cond_set:\n",
    "                    if rule.confidence < new_rule.confidence:\n",
    "                        self.rules.remove(rule)\n",
    "                        self.rules.add(new_rule)\n",
    "                    return\n",
    "            self.rules.add(new_rule)\n",
    "    \n",
    "    def gen_rules(self, frequent_rules, minsup, minconf):\n",
    "        for rules in frequent_rules.rule_dict.values():\n",
    "            for rule in rules:\n",
    "                self._add(rule, minsup, minconf)\n",
    "\n",
    "    def prune_rules(self, dataset):\n",
    "        for rule in self.rules:\n",
    "            pruned_rule = prune(rule, dataset)\n",
    "\n",
    "            existed = False\n",
    "            for rule in self.pruned_rules:\n",
    "                if rule.label == pruned_rule.label:\n",
    "                    if rule.cond_set == pruned_rule.cond_set:\n",
    "                        existed = True\n",
    "                        break\n",
    "            \n",
    "            if not existed:\n",
    "                self.prune_rules.add(pruned_rule)\n",
    "    \n",
    "    def append(self, car, minsup, minconf):\n",
    "        for rule in car.rules:\n",
    "            self._add(rule, minsup, minconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_satisfy(datacase, rule):\n",
    "    for item in rule.cond_set:\n",
    "        if datacase[item] != rule.cond_set[item]:\n",
    "            return None\n",
    "    if datacase[-1] == rule.class_label:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "    This class is our classifier. The rule_list and default_class are useful for outer code.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.rule_list = list()\n",
    "        self.default_class = None\n",
    "        self._error_list = list()\n",
    "        self._default_class_list = list()\n",
    "\n",
    "    # insert a rule into rule_list, then choose a default class, and calculate the errors (see line 8, 10 & 11)\n",
    "    def insert(self, rule, dataset):\n",
    "        self.rule_list.append(rule)             # insert r at the end of C\n",
    "        self._select_default_class(dataset)     # select a default class for the current C\n",
    "        self._compute_error(dataset)            # compute the total number of errors of C\n",
    "\n",
    "    # select the majority class in the remaining data\n",
    "    def _select_default_class(self, dataset):\n",
    "        class_column = [x[-1] for x in dataset]\n",
    "        class_label = set(class_column)\n",
    "        max = 0\n",
    "        current_default_class = None\n",
    "        for label in class_label:\n",
    "            if class_column.count(label) >= max:\n",
    "                max = class_column.count(label)\n",
    "                current_default_class = label\n",
    "        self._default_class_list.append(current_default_class)\n",
    "\n",
    "    # compute the sum of errors\n",
    "    def _compute_error(self, dataset):\n",
    "        if len(dataset) <= 0:\n",
    "            self._error_list.append(sys.maxsize)\n",
    "            return\n",
    "\n",
    "        error_number = 0\n",
    "\n",
    "        # the number of errors that have been made by all the selected rules in C\n",
    "        for case in dataset:\n",
    "            is_cover = False\n",
    "            for rule in self.rule_list:\n",
    "                if is_satisfy(case, rule):\n",
    "                    is_cover = True\n",
    "                    break\n",
    "            if not is_cover:\n",
    "                error_number += 1\n",
    "\n",
    "        # the number of errors to be made by the default class in the training set\n",
    "        class_column = [x[-1] for x in dataset]\n",
    "        error_number += len(class_column) - class_column.count(self._default_class_list[-1])\n",
    "        self._error_list.append(error_number)\n",
    "\n",
    "    # see line 14 and 15, to get the final classifier\n",
    "    def discard(self):\n",
    "        # find the first rule p in C with the lowest total number of errors and drop all the rules after p in C\n",
    "        index = self._error_list.index(min(self._error_list))\n",
    "        self.rule_list = self.rule_list[:(index+1)]\n",
    "        self._error_list = None\n",
    "\n",
    "        # assign the default class associated with p to default_class\n",
    "        self.default_class = self._default_class_list[index]\n",
    "        self._default_class_list = None\n",
    "\n",
    "    # just print out all selected rules and default class in our classifier\n",
    "    def print(self):\n",
    "        for rule in self.rule_list:\n",
    "            rule.print_rule()\n",
    "        print(\"default_class:\", self.default_class)\n",
    "\n",
    "\n",
    "# sort the set of generated rules car according to the relation \">\", return the sorted rule list\n",
    "def sort(car):\n",
    "    def cmp_method(a, b):\n",
    "        if a.confidence < b.confidence:     # 1. the confidence of ri > rj\n",
    "            return 1\n",
    "        elif a.confidence == b.confidence:\n",
    "            if a.support < b.support:       # 2. their confidences are the same, but support of ri > rj\n",
    "                return 1\n",
    "            elif a.support == b.support:\n",
    "                if len(a.cond_set) < len(b.cond_set):   # 3. both confidence & support are the same, ri earlier than rj\n",
    "                    return -1\n",
    "                elif len(a.cond_set) == len(b.cond_set):\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    rule_list = list(car.rules)\n",
    "    rule_list.sort(key=cmp_to_key(cmp_method))\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "# main method of CBA-CB: M1\n",
    "def classifier_builder_m1(cars, dataset):\n",
    "    classifier = Classifier()\n",
    "    cars_list = sort(cars)\n",
    "    for rule in cars_list:\n",
    "        temp = []\n",
    "        mark = False\n",
    "        for i in range(len(dataset)):\n",
    "            is_satisfy_value = is_satisfy(dataset[i], rule)\n",
    "            if is_satisfy_value is not None:\n",
    "                temp.append(i)\n",
    "                if is_satisfy_value:\n",
    "                    mark = True\n",
    "        if mark:\n",
    "            temp_dataset = list(dataset)\n",
    "            for index in temp:\n",
    "                temp_dataset[index] = []\n",
    "            while [] in temp_dataset:\n",
    "                temp_dataset.remove([])\n",
    "            dataset = temp_dataset\n",
    "            classifier.insert(rule, dataset)\n",
    "    classifier.discard()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the error rate of the classifier on the dataset\n",
    "def get_error_rate(classifier, dataset):\n",
    "    size = len(dataset)\n",
    "    error_number = 0\n",
    "    for case in dataset:\n",
    "        is_satisfy_value = False\n",
    "        for rule in classifier.rule_list:\n",
    "            is_satisfy_value = is_satisfy(case, rule)\n",
    "            if is_satisfy_value == True:\n",
    "                break\n",
    "        if is_satisfy_value == False:\n",
    "            if classifier.default_class != case[-1]:\n",
    "                error_number += 1\n",
    "    return error_number / size\n",
    "\n",
    "\n",
    "# 10-fold cross-validations on CBA (M1) without pruning\n",
    "def cross_validate_m1_without_prune(dataset, minsup=0.01, minconf=0.5, quiet=False):\n",
    "\n",
    "    block_size = int(len(dataset) / 10)\n",
    "    split_point = [k * block_size for k in range(0, 10)]\n",
    "    split_point.append(len(dataset))\n",
    "    \n",
    "    cba_rg_total_runtime = 0\n",
    "    cba_cb_total_runtime = 0\n",
    "    total_car_number = 0\n",
    "    total_classifier_rule_num = 0\n",
    "    error_total_rate = 0\n",
    "\n",
    "    for k in range(len(split_point)-1):\n",
    "        if not quiet:\n",
    "            print(\"\\nRound %d:\" % k)\n",
    "\n",
    "        training_dataset = dataset[:split_point[k]] + dataset[split_point[k+1]:]\n",
    "        test_dataset = dataset[split_point[k]:split_point[k+1]]\n",
    "\n",
    "        start_time = time.time()\n",
    "        cars = rule_generator(training_dataset, minsup, minconf)\n",
    "        end_time = time.time()\n",
    "        cba_rg_runtime = end_time - start_time\n",
    "        cba_rg_total_runtime += cba_rg_runtime\n",
    "\n",
    "        start_time = time.time()\n",
    "        classifier_m1 = classifier_builder_m1(cars, training_dataset)\n",
    "        end_time = time.time()\n",
    "        cba_cb_runtime = end_time - start_time\n",
    "        cba_cb_total_runtime += cba_cb_runtime\n",
    "\n",
    "        error_rate = get_error_rate(classifier_m1, test_dataset)\n",
    "        error_total_rate += error_rate\n",
    "\n",
    "        total_car_number += len(cars.rules)\n",
    "        total_classifier_rule_num += len(classifier_m1.rule_list)\n",
    "\n",
    "        if not quiet:\n",
    "            print(f\"CBA's error rate without pruning: {error_rate:.2%}\")\n",
    "            print(f\"No. of CARs without pruning: {len(cars.rules)}\")\n",
    "            print(f\"CBA-RG's run time without pruning: {cba_rg_runtime:.3f} s\")\n",
    "            print(f\"CBA-CB M1's run time without pruning: {cba_cb_runtime:.3f} s\")\n",
    "            print(f\"No. of rules in classifier of CBA-CB M1 without pruning: {len(classifier_m1.rule_list)}\")\n",
    "\n",
    "    if not quiet:    \n",
    "        print(f\"\\nAverage CBA's error rate without pruning: {(error_total_rate / 10):.2%}\")\n",
    "        print(f\"Average No. of CARs without pruning: {int(total_car_number / 10)}\")\n",
    "        print(f\"Average CBA-RG's run time without pruning: {(cba_rg_total_runtime / 10):.3f} s\")\n",
    "        print(f\"Average CBA-CB M1's run time without pruning: {(cba_cb_total_runtime / 10):.3f} s\")\n",
    "        print(f\"Average No. of rules in classifier of CBA-CB M1 without pruning: {int(total_classifier_rule_num / 10)}\")\n",
    "\n",
    "    return error_total_rate / 10, int(total_car_number / 10), (cba_rg_total_runtime / 10), (cba_cb_total_runtime / 10), int(total_classifier_rule_num / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying German dataset\n",
      "Classifying Australian dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_funcs = [German, Australian, Crx, Hepatitis, Ionosphere, Pumpkin, Mushroom, Diabetes]\n",
    "stats_list = {'dataset':[], 'error_rate':[], 'CBA_count':[], 'CBA_RG_runtime':[], 'CBA_CB_runtime':[], 'rule_count':[] }\n",
    "for dataset_getter in dataset_funcs:\n",
    "    print(f\"Classifying {dataset_getter.__name__} dataset\")\n",
    "    avg_err_rate, avg_car_cnt, avg_rg_runtime, avg_cb_runtime, avg_rule_cnt = cross_validate_m1_without_prune(dataset_getter().values.tolist(), quiet=True)\n",
    "    stats_list['dataset'].append(dataset_getter.__name__)\n",
    "    stats_list['error_rate'].append(avg_err_rate)\n",
    "    stats_list['CBA_count'].append(avg_car_cnt)\n",
    "    stats_list['CBA_RG_runtime'].append(avg_rg_runtime)\n",
    "    stats_list['CBA_CB_runtime'].append(avg_cb_runtime)\n",
    "    stats_list['rule_count'].append(avg_rule_cnt)\n",
    "pd.DataFrame(stats_list).to_csv(os.path.join(OUTPUT, 'CBA_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb1816ef6abe37709700f59d296504afbde16240c11088d3d5ef633232cba45c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
