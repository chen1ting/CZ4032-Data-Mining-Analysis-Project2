{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from functools import cmp_to_key\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"german.data\", delim_whitespace = True, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the datatype of columns in the dataset\n",
    "# 1 = numerical, 0 = qualitative\n",
    "datatypes = [\n",
    "    0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
    "    1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11</td>\n",
       "      <td>6</td>\n",
       "      <td>A34</td>\n",
       "      <td>A43</td>\n",
       "      <td>1169</td>\n",
       "      <td>A65</td>\n",
       "      <td>A75</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>67</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A12</td>\n",
       "      <td>48</td>\n",
       "      <td>A32</td>\n",
       "      <td>A43</td>\n",
       "      <td>5951</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>2</td>\n",
       "      <td>A92</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>22</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14</td>\n",
       "      <td>12</td>\n",
       "      <td>A34</td>\n",
       "      <td>A46</td>\n",
       "      <td>2096</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>49</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A172</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A11</td>\n",
       "      <td>42</td>\n",
       "      <td>A32</td>\n",
       "      <td>A42</td>\n",
       "      <td>7882</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A103</td>\n",
       "      <td>...</td>\n",
       "      <td>A122</td>\n",
       "      <td>45</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A11</td>\n",
       "      <td>24</td>\n",
       "      <td>A33</td>\n",
       "      <td>A40</td>\n",
       "      <td>4870</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>3</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A124</td>\n",
       "      <td>53</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1    2    3     4    5    6   7    8     9   ...    11  12    13  \\\n",
       "0  A11   6  A34  A43  1169  A65  A75   4  A93  A101  ...  A121  67  A143   \n",
       "1  A12  48  A32  A43  5951  A61  A73   2  A92  A101  ...  A121  22  A143   \n",
       "2  A14  12  A34  A46  2096  A61  A74   2  A93  A101  ...  A121  49  A143   \n",
       "3  A11  42  A32  A42  7882  A61  A74   2  A93  A103  ...  A122  45  A143   \n",
       "4  A11  24  A33  A40  4870  A61  A73   3  A93  A101  ...  A124  53  A143   \n",
       "\n",
       "     14 15    16 17    18    19 20  \n",
       "0  A152  2  A173  1  A192  A201  1  \n",
       "1  A152  1  A173  1  A191  A201  2  \n",
       "2  A152  1  A172  2  A191  A201  1  \n",
       "3  A153  1  A173  2  A191  A201  1  \n",
       "4  A153  2  A173  2  A191  A201  2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in column column_no, when missing values ration below 50%.\n",
    "# data: original data list\n",
    "# column_no: identify the column No. of that to be filled\n",
    "def fill_missing_values(data, datatypes, column_no):\n",
    "    size = len(data[column_no])\n",
    "    # categorical\n",
    "    if (datatypes[column_no]) == 0:\n",
    "        for i in range(size):\n",
    "            if data.iloc[i, column_no] == '?':\n",
    "                data.iloc[i, column_no] = data[i].mode().values[0]\n",
    "    # numerical\n",
    "    elif (datatypes[column_no]) == 1:\n",
    "        for i in range(size):\n",
    "            if data.iloc[i, column_no] == '?':\n",
    "                data.iloc[i, column_no] = data[i].median()\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_discretization_data(data, column_no):\n",
    "    if(len(pd.unique(data.iloc[:,column_no])) <= 5):\n",
    "        return data\n",
    "    k = 5\n",
    "    k_model = KMeans(n_clusters=k)\n",
    "    k_model.fit(data.iloc[:,column_no].values.reshape(len(data[column_no]), 1))\n",
    "    c = pd.DataFrame(k_model.cluster_centers_, columns = list(\"a\")).sort_values(by = \"a\")\n",
    "    w = c.rolling(2).mean().iloc[1:]\n",
    "    w = np.asarray(w.values)\n",
    "    w = [i[0] for i in w]\n",
    "    w = [0] + w + [data.iloc[:,column_no].max()]\n",
    "    data.iloc[:,column_no] = pd.cut(data.iloc[:,column_no], w, labels = range(k))\n",
    "    return data\n",
    "\n",
    "\n",
    "# Replace categorical values with a positive integer.\n",
    "# data: original data table\n",
    "# column_no: identify which column to be processed\n",
    "def replace_categorical(data, column_no):\n",
    "    size = len(data)\n",
    "    classes = set(data[column_no])\n",
    "    classes_no = dict([(label, 0) for label in classes])\n",
    "    j = 1\n",
    "    for i in classes:\n",
    "        classes_no[i] = j\n",
    "        j += 1\n",
    "    for i in range(size):\n",
    "        data.iloc[i, column_no] = classes_no[data[column_no][i]]\n",
    "    return data, classes_no\n",
    "\n",
    "\n",
    "# Discard all the column with its column_no in discard_list\n",
    "# data: original data set\n",
    "# discard_list: a list of column No. of the columns to be discarded\n",
    "def discard(data, discard_list):\n",
    "    size = len(data)\n",
    "    length = len(data[0])\n",
    "    data_result = pd.DataFrame()\n",
    "    for j in range(length):\n",
    "        if j not in discard_list:\n",
    "            data_result.append(data[j])\n",
    "    return data_result\n",
    "\n",
    "\n",
    "# Main method here, see Description in detail\n",
    "# data: original data table\n",
    "# attribute: a list of the name of attribute\n",
    "# value_type: a list identifying the type of each column\n",
    "# Returned value: a data table after process\n",
    "def pre_process(data, datatypes):\n",
    "    column_num = len(datatypes)\n",
    "    size = len(data)\n",
    "    discard_list = []\n",
    "    for i in range(0, column_num):\n",
    "        # process missing values\n",
    "        val_count = data[i].value_counts()\n",
    "        if(\"?\" in val_count):\n",
    "            missing_values_ratio = val_count[\"?\"] / size\n",
    "            if missing_values_ratio > 0.5:\n",
    "                discard_list.append(i)\n",
    "                continue\n",
    "            elif missing_values_ratio > 0:\n",
    "                data[i] = fill_missing_values(data, i)\n",
    "                print(\"fill missing value\")\n",
    "\n",
    "        # discretization\n",
    "        if datatypes[i] == 1:\n",
    "            data = get_discretization_data(data, i)\n",
    "        elif datatypes[i] == 0:\n",
    "            data, class_no= replace_categorical(data, i)\n",
    "\n",
    "    # discard\n",
    "    if len(discard_list) > 0:\n",
    "        data = discard(data, discard_list)\n",
    "        print(\"discard:\", discard_list)             # print out discard list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11</td>\n",
       "      <td>6</td>\n",
       "      <td>A34</td>\n",
       "      <td>A43</td>\n",
       "      <td>1169</td>\n",
       "      <td>A65</td>\n",
       "      <td>A75</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>67</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A12</td>\n",
       "      <td>48</td>\n",
       "      <td>A32</td>\n",
       "      <td>A43</td>\n",
       "      <td>5951</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>2</td>\n",
       "      <td>A92</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>22</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A14</td>\n",
       "      <td>12</td>\n",
       "      <td>A34</td>\n",
       "      <td>A46</td>\n",
       "      <td>2096</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A121</td>\n",
       "      <td>49</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A172</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A11</td>\n",
       "      <td>42</td>\n",
       "      <td>A32</td>\n",
       "      <td>A42</td>\n",
       "      <td>7882</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>A103</td>\n",
       "      <td>...</td>\n",
       "      <td>A122</td>\n",
       "      <td>45</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A11</td>\n",
       "      <td>24</td>\n",
       "      <td>A33</td>\n",
       "      <td>A40</td>\n",
       "      <td>4870</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>3</td>\n",
       "      <td>A93</td>\n",
       "      <td>A101</td>\n",
       "      <td>...</td>\n",
       "      <td>A124</td>\n",
       "      <td>53</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1    2    3     4    5    6   7    8     9   ...    11  12    13  \\\n",
       "0  A11   6  A34  A43  1169  A65  A75   4  A93  A101  ...  A121  67  A143   \n",
       "1  A12  48  A32  A43  5951  A61  A73   2  A92  A101  ...  A121  22  A143   \n",
       "2  A14  12  A34  A46  2096  A61  A74   2  A93  A101  ...  A121  49  A143   \n",
       "3  A11  42  A32  A42  7882  A61  A74   2  A93  A103  ...  A122  45  A143   \n",
       "4  A11  24  A33  A40  4870  A61  A73   3  A93  A101  ...  A124  53  A143   \n",
       "\n",
       "     14 15    16 17    18    19 20  \n",
       "0  A152  2  A173  1  A192  A201  1  \n",
       "1  A152  1  A173  1  A191  A201  2  \n",
       "2  A152  1  A172  2  A191  A201  1  \n",
       "3  A153  1  A173  2  A191  A201  1  \n",
       "4  A153  2  A173  2  A191  A201  2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0  1  2  3  4  5  6   7  8  9   ...  11 12 13 14 15  16 17  18 19 20\n",
       "0  2  0  1  5  0  4  4   4  4  3  ...   3  4  2  2  2   4  1   2  1  1\n",
       "1  3  4  4  5  3  5  1   2  1  3  ...   3  0  2  2  1   4  1   1  1  2\n",
       "2  4  0  1  6  1  5  2   2  4  3  ...   3  3  2  2  1   1  2   1  1  1\n",
       "3  2  3  4  9  3  5  2   2  4  2  ...   1  3  2  1  1   4  2   1  1  1\n",
       "4  2  2  2  4  2  5  1   3  4  3  ...   2  3  2  1  2   4  2   1  1  2\n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = pre_process(data.copy(), datatypes)\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleItem:\n",
    "    \"\"\"\n",
    "    cond_set: a dict with following fashion:\n",
    "            {item name: value, item name: value, ...}\n",
    "        e.g.\n",
    "            {A: 1, B: 1} (A, B are name of columns, here called \"item\", and in our code should be numerical index\n",
    "                          but not string)\n",
    "    class_label: just to identify the class it belongs to.\n",
    "    dataset: a list returned by read method. (see read.py)\n",
    "    cond_sup_count, rule_sup_count, support and confidence are number.\n",
    "    \"\"\"\n",
    "    def __init__(self, cond_set, class_label, dataset):\n",
    "        self.cond_set = cond_set\n",
    "        self.class_label = class_label\n",
    "        self.cond_sup_count, self.rule_sup_count = self._get_sup_count(dataset)\n",
    "        self.support = self._get_support(len(dataset))\n",
    "        self.confidence = self._get_confidence()\n",
    "\n",
    "    # calculate condsupCount and rulesupCount\n",
    "    def _get_sup_count(self, dataset):\n",
    "        cond_sup_count = 0\n",
    "        rule_sup_count = 0\n",
    "        for case in dataset:\n",
    "            is_contained = True\n",
    "            for index in self.cond_set:\n",
    "                if self.cond_set[index] != case[index]:\n",
    "                    is_contained = False\n",
    "                    break\n",
    "            if is_contained:\n",
    "                cond_sup_count += 1\n",
    "                if self.class_label == case[-1]:\n",
    "                    rule_sup_count += 1\n",
    "        return cond_sup_count, rule_sup_count\n",
    "\n",
    "    # calculate support count\n",
    "    def _get_support(self, dataset_size):\n",
    "        return self.rule_sup_count / dataset_size\n",
    "\n",
    "    # calculate confidence\n",
    "    def _get_confidence(self):\n",
    "        if self.cond_sup_count != 0:\n",
    "            return self.rule_sup_count / self.cond_sup_count\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # print out the ruleitem\n",
    "    def print(self):\n",
    "        cond_set_output = ''\n",
    "        for item in self.cond_set:\n",
    "            cond_set_output += '(' + str(item) + ', ' + str(self.cond_set[item]) + '), '\n",
    "        cond_set_output = cond_set_output[:-2]\n",
    "        print('<({' + cond_set_output + '}, ' + str(self.cond_sup_count) + '), (' +\n",
    "              '(class, ' + str(self.class_label) + '), ' + str(self.rule_sup_count) + ')>')\n",
    "\n",
    "    # print out rule\n",
    "    def print_rule(self):\n",
    "        cond_set_output = ''\n",
    "        for item in self.cond_set:\n",
    "            cond_set_output += '(' + str(item) + ', ' + str(self.cond_set[item]) + '), '\n",
    "        cond_set_output = '{' + cond_set_output[:-2] + '}'\n",
    "        print(cond_set_output + ' -> (class, ' + str(self.class_label) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequentRuleitems:\n",
    "    \"\"\"\n",
    "    A set of frequent k-ruleitems, just using set.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.frequent_ruleitems_set = set()\n",
    "\n",
    "    # get size of set\n",
    "    def get_size(self):\n",
    "        return len(self.frequent_ruleitems_set)\n",
    "\n",
    "    # add a new ruleitem into set\n",
    "    def add(self, rule_item):\n",
    "        is_existed = False\n",
    "        for item in self.frequent_ruleitems_set:\n",
    "            if item.class_label == rule_item.class_label:\n",
    "                if item.cond_set == rule_item.cond_set:\n",
    "                    is_existed = True\n",
    "                    break\n",
    "        if not is_existed:\n",
    "            self.frequent_ruleitems_set.add(rule_item)\n",
    "\n",
    "    # append set of ruleitems\n",
    "    def append(self, sets):\n",
    "        for item in sets.frequent_ruleitems:\n",
    "            self.add(item)\n",
    "\n",
    "    # print out all frequent ruleitems\n",
    "    def print(self):\n",
    "        for item in self.frequent_ruleitems_set:\n",
    "            item.print()\n",
    "\n",
    "\n",
    "class Car:\n",
    "    \"\"\"\n",
    "    Class Association Rules (Car). If some ruleitems has the same condset, the ruleitem with the highest confidence is\n",
    "    chosen as the Possible Rule (PR). If there're more than one ruleitem with the same highest confidence, we randomly\n",
    "    select one ruleitem.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.rules = set()\n",
    "        self.pruned_rules = set()\n",
    "\n",
    "    # print out all rules\n",
    "    def print_rule(self):\n",
    "        for item in self.rules:\n",
    "            item.print_rule()\n",
    "\n",
    "    # print out all pruned rules\n",
    "    def print_pruned_rule(self):\n",
    "        for item in self.pruned_rules:\n",
    "            item.print_rule()\n",
    "\n",
    "    # add a new rule (frequent & accurate), save the ruleitem with the highest confidence when having the same condset\n",
    "    def _add(self, rule_item, minsup, minconf):\n",
    "        if rule_item.support >= minsup and rule_item.confidence >= minconf:\n",
    "            if rule_item in self.rules:\n",
    "                return\n",
    "            for item in self.rules:\n",
    "                if item.cond_set == rule_item.cond_set and item.confidence < rule_item.confidence:\n",
    "                    self.rules.remove(item)\n",
    "                    self.rules.add(rule_item)\n",
    "                    return\n",
    "                elif item.cond_set == rule_item.cond_set and item.confidence >= rule_item.confidence:\n",
    "                    return\n",
    "            self.rules.add(rule_item)\n",
    "\n",
    "    # convert frequent ruleitems into car\n",
    "    def gen_rules(self, frequent_ruleitems, minsup, minconf):\n",
    "        for item in frequent_ruleitems.frequent_ruleitems_set:\n",
    "            self._add(item, minsup, minconf)\n",
    "\n",
    "    # prune rules\n",
    "    def prune_rules(self, dataset):\n",
    "        for rule in self.rules:\n",
    "            pruned_rule = prune(rule, dataset)\n",
    "\n",
    "            is_existed = False\n",
    "            for rule in self.pruned_rules:\n",
    "                if rule.class_label == pruned_rule.class_label:\n",
    "                    if rule.cond_set == pruned_rule.cond_set:\n",
    "                        is_existed = True\n",
    "                        break\n",
    "\n",
    "            if not is_existed:\n",
    "                self.pruned_rules.add(pruned_rule)\n",
    "\n",
    "    # union new car into rules list\n",
    "    def append(self, car, minsup, minconf):\n",
    "        for item in car.rules:\n",
    "            self._add(item, minsup, minconf)\n",
    "\n",
    "\n",
    "# try to prune rule\n",
    "def prune(rule, dataset):\n",
    "    import sys\n",
    "    min_rule_error = sys.maxsize\n",
    "    pruned_rule = rule\n",
    "\n",
    "    # prune rule recursively\n",
    "    def find_prune_rule(this_rule):\n",
    "        nonlocal min_rule_error\n",
    "        nonlocal pruned_rule\n",
    "\n",
    "        # calculate how many errors the rule r make in the dataset\n",
    "        def errors_of_rule(r):\n",
    "\n",
    "            errors_number = 0\n",
    "            for case in dataset:\n",
    "                if is_satisfy(case, r) == False:\n",
    "                    errors_number += 1\n",
    "            return errors_number\n",
    "\n",
    "        rule_error = errors_of_rule(this_rule)\n",
    "        if rule_error < min_rule_error:\n",
    "            min_rule_error = rule_error\n",
    "            pruned_rule = this_rule\n",
    "        this_rule_cond_set = list(this_rule.cond_set)\n",
    "        if len(this_rule_cond_set) >= 2:\n",
    "            for attribute in this_rule_cond_set:\n",
    "                temp_cond_set = dict(this_rule.cond_set)\n",
    "                temp_cond_set.pop(attribute)\n",
    "                temp_rule = RuleItem(temp_cond_set, this_rule.class_label, dataset)\n",
    "                temp_rule_error = errors_of_rule(temp_rule)\n",
    "                if temp_rule_error <= min_rule_error:\n",
    "                    min_rule_error = temp_rule_error\n",
    "                    pruned_rule = temp_rule\n",
    "                    if len(temp_cond_set) >= 2:\n",
    "                        find_prune_rule(temp_rule)\n",
    "\n",
    "    find_prune_rule(rule)\n",
    "    return pruned_rule\n",
    "\n",
    "\n",
    "# invoked by candidate_gen, join two items to generate candidate\n",
    "def join(item1, item2, dataset):\n",
    "    if item1.class_label != item2.class_label:\n",
    "        return None\n",
    "    category1 = set(item1.cond_set)\n",
    "    category2 = set(item2.cond_set)\n",
    "    if category1 == category2:\n",
    "        return None\n",
    "    intersect = category1 & category2\n",
    "    for item in intersect:\n",
    "        if item1.cond_set[item] != item2.cond_set[item]:\n",
    "            return None\n",
    "    category = category1 | category2\n",
    "    new_cond_set = dict()\n",
    "    for item in category:\n",
    "        if item in category1:\n",
    "            new_cond_set[item] = item1.cond_set[item]\n",
    "        else:\n",
    "            new_cond_set[item] = item2.cond_set[item]\n",
    "    new_ruleitem = RuleItem(new_cond_set, item1.class_label, dataset)\n",
    "    return new_ruleitem\n",
    "\n",
    "\n",
    "# similar to Apriori-gen in algorithm Apriori\n",
    "def candidate_gen(frequent_ruleitems, dataset):\n",
    "    returned_frequent_ruleitems = FrequentRuleitems()\n",
    "    for item1 in frequent_ruleitems.frequent_ruleitems_set:\n",
    "        for item2 in frequent_ruleitems.frequent_ruleitems_set:\n",
    "            new_ruleitem = join(item1, item2, dataset)\n",
    "            if new_ruleitem:\n",
    "                returned_frequent_ruleitems.add(new_ruleitem)\n",
    "                if returned_frequent_ruleitems.get_size() >= 1000:      # not allow to store more than 1000 ruleitems\n",
    "                    return returned_frequent_ruleitems\n",
    "    return returned_frequent_ruleitems\n",
    "\n",
    "\n",
    "# main method, implementation of CBA-RG algorithm\n",
    "def rule_generator(dataset, minsup, minconf):\n",
    "    frequent_ruleitems = FrequentRuleitems()\n",
    "    car = Car()\n",
    "\n",
    "    # get large 1-ruleitems and generate rules\n",
    "    class_label = set([x[-1] for x in dataset])\n",
    "    for column in range(0, len(dataset[0])-1):\n",
    "        distinct_value = set([x[column] for x in dataset])\n",
    "        for value in distinct_value:\n",
    "            cond_set = {column: value}\n",
    "            for classes in class_label:\n",
    "                rule_item = RuleItem(cond_set, classes, dataset)\n",
    "                if rule_item.support >= minsup:\n",
    "                    frequent_ruleitems.add(rule_item)\n",
    "    car.gen_rules(frequent_ruleitems, minsup, minconf)\n",
    "    cars = car\n",
    "\n",
    "    last_cars_number = 0\n",
    "    current_cars_number = len(cars.rules)\n",
    "    while frequent_ruleitems.get_size() > 0 and current_cars_number <= 2000 and \\\n",
    "                    (current_cars_number - last_cars_number) >= 10:\n",
    "        candidate = candidate_gen(frequent_ruleitems, dataset)\n",
    "        frequent_ruleitems = FrequentRuleitems()\n",
    "        car = Car()\n",
    "        for item in candidate.frequent_ruleitems_set:\n",
    "            if item.support >= minsup:\n",
    "                frequent_ruleitems.add(item)\n",
    "        car.gen_rules(frequent_ruleitems, minsup, minconf)\n",
    "        cars.append(car, minsup, minconf)\n",
    "        last_cars_number = current_cars_number\n",
    "        current_cars_number = len(cars.rules)\n",
    "\n",
    "    return cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAR:\n",
    "    def __init__(self):\n",
    "        self.rules = set()\n",
    "        self.pruned_rules = set()\n",
    "\n",
    "    def print_rule(self):\n",
    "        for rule in self.rules:\n",
    "            rule.print()\n",
    "    \n",
    "    def print_pruned_rules(self):\n",
    "        for rule in self.pruned_rules:\n",
    "            rule.print()\n",
    "\n",
    "    def _add(self, new_rule, minsup, minconf):\n",
    "        if new_rule.support >= minsup and new_rule.confidence >= minconf:\n",
    "            if new_rule in self.rules:\n",
    "                return\n",
    "            for rule in self.rules:\n",
    "                if rule.cond_set == new_rule.cond_set:\n",
    "                    if rule.confidence < new_rule.confidence:\n",
    "                        self.rules.remove(rule)\n",
    "                        self.rules.add(new_rule)\n",
    "                    return\n",
    "            self.rules.add(new_rule)\n",
    "    \n",
    "    def gen_rules(self, frequent_rules, minsup, minconf):\n",
    "        for rules in frequent_rules.rule_dict.values():\n",
    "            for rule in rules:\n",
    "                self._add(rule, minsup, minconf)\n",
    "\n",
    "    def prune_rules(self, dataset):\n",
    "        for rule in self.rules:\n",
    "            pruned_rule = prune(rule, dataset)\n",
    "\n",
    "            existed = False\n",
    "            for rule in self.pruned_rules:\n",
    "                if rule.label == pruned_rule.label:\n",
    "                    if rule.cond_set == pruned_rule.cond_set:\n",
    "                        existed = True\n",
    "                        break\n",
    "            \n",
    "            if not existed:\n",
    "                self.prune_rules.add(pruned_rule)\n",
    "    \n",
    "    def append(self, car, minsup, minconf):\n",
    "        for rule in car.rules:\n",
    "            self._add(rule, minsup, minconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_satisfy(datacase, rule):\n",
    "    for item in rule.cond_set:\n",
    "        if datacase[item] != rule.cond_set[item]:\n",
    "            return None\n",
    "    if datacase[-1] == rule.class_label:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "    This class is our classifier. The rule_list and default_class are useful for outer code.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.rule_list = list()\n",
    "        self.default_class = None\n",
    "        self._error_list = list()\n",
    "        self._default_class_list = list()\n",
    "\n",
    "    # insert a rule into rule_list, then choose a default class, and calculate the errors (see line 8, 10 & 11)\n",
    "    def insert(self, rule, dataset):\n",
    "        self.rule_list.append(rule)             # insert r at the end of C\n",
    "        self._select_default_class(dataset)     # select a default class for the current C\n",
    "        self._compute_error(dataset)            # compute the total number of errors of C\n",
    "\n",
    "    # select the majority class in the remaining data\n",
    "    def _select_default_class(self, dataset):\n",
    "        class_column = [x[-1] for x in dataset]\n",
    "        class_label = set(class_column)\n",
    "        max = 0\n",
    "        current_default_class = None\n",
    "        for label in class_label:\n",
    "            if class_column.count(label) >= max:\n",
    "                max = class_column.count(label)\n",
    "                current_default_class = label\n",
    "        self._default_class_list.append(current_default_class)\n",
    "\n",
    "    # compute the sum of errors\n",
    "    def _compute_error(self, dataset):\n",
    "        if len(dataset) <= 0:\n",
    "            self._error_list.append(sys.maxsize)\n",
    "            return\n",
    "\n",
    "        error_number = 0\n",
    "\n",
    "        # the number of errors that have been made by all the selected rules in C\n",
    "        for case in dataset:\n",
    "            is_cover = False\n",
    "            for rule in self.rule_list:\n",
    "                if is_satisfy(case, rule):\n",
    "                    is_cover = True\n",
    "                    break\n",
    "            if not is_cover:\n",
    "                error_number += 1\n",
    "\n",
    "        # the number of errors to be made by the default class in the training set\n",
    "        class_column = [x[-1] for x in dataset]\n",
    "        error_number += len(class_column) - class_column.count(self._default_class_list[-1])\n",
    "        self._error_list.append(error_number)\n",
    "\n",
    "    # see line 14 and 15, to get the final classifier\n",
    "    def discard(self):\n",
    "        # find the first rule p in C with the lowest total number of errors and drop all the rules after p in C\n",
    "        index = self._error_list.index(min(self._error_list))\n",
    "        self.rule_list = self.rule_list[:(index+1)]\n",
    "        self._error_list = None\n",
    "\n",
    "        # assign the default class associated with p to default_class\n",
    "        self.default_class = self._default_class_list[index]\n",
    "        self._default_class_list = None\n",
    "\n",
    "    # just print out all selected rules and default class in our classifier\n",
    "    def print(self):\n",
    "        for rule in self.rule_list:\n",
    "            rule.print_rule()\n",
    "        print(\"default_class:\", self.default_class)\n",
    "\n",
    "\n",
    "# sort the set of generated rules car according to the relation \">\", return the sorted rule list\n",
    "def sort(car):\n",
    "    def cmp_method(a, b):\n",
    "        if a.confidence < b.confidence:     # 1. the confidence of ri > rj\n",
    "            return 1\n",
    "        elif a.confidence == b.confidence:\n",
    "            if a.support < b.support:       # 2. their confidences are the same, but support of ri > rj\n",
    "                return 1\n",
    "            elif a.support == b.support:\n",
    "                if len(a.cond_set) < len(b.cond_set):   # 3. both confidence & support are the same, ri earlier than rj\n",
    "                    return -1\n",
    "                elif len(a.cond_set) == len(b.cond_set):\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    rule_list = list(car.rules)\n",
    "    rule_list.sort(key=cmp_to_key(cmp_method))\n",
    "    return rule_list\n",
    "\n",
    "\n",
    "# main method of CBA-CB: M1\n",
    "def classifier_builder_m1(cars, dataset):\n",
    "    classifier = Classifier()\n",
    "    cars_list = sort(cars)\n",
    "    for rule in cars_list:\n",
    "        temp = []\n",
    "        mark = False\n",
    "        for i in range(len(dataset)):\n",
    "            is_satisfy_value = is_satisfy(dataset[i], rule)\n",
    "            if is_satisfy_value is not None:\n",
    "                temp.append(i)\n",
    "                if is_satisfy_value:\n",
    "                    mark = True\n",
    "        if mark:\n",
    "            temp_dataset = list(dataset)\n",
    "            for index in temp:\n",
    "                temp_dataset[index] = []\n",
    "            while [] in temp_dataset:\n",
    "                temp_dataset.remove([])\n",
    "            dataset = temp_dataset\n",
    "            classifier.insert(rule, dataset)\n",
    "    classifier.discard()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = processed_data[:800]\n",
    "test_data = processed_data[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the error rate of the classifier on the dataset\n",
    "def get_error_rate(classifier, dataset):\n",
    "    size = len(dataset)\n",
    "    error_number = 0\n",
    "    for case in dataset:\n",
    "        is_satisfy_value = False\n",
    "        for rule in classifier.rule_list:\n",
    "            is_satisfy_value = is_satisfy(case, rule)\n",
    "            if is_satisfy_value == True:\n",
    "                break\n",
    "        if is_satisfy_value == False:\n",
    "            if classifier.default_class != case[-1]:\n",
    "                error_number += 1\n",
    "    return error_number / size\n",
    "\n",
    "\n",
    "# 10-fold cross-validations on CBA (M1) without pruning\n",
    "def cross_validate_m1_without_prune(dataset, minsup=0.01, minconf=0.5):\n",
    "\n",
    "    block_size = int(len(dataset) / 10)\n",
    "    split_point = [k * block_size for k in range(0, 10)]\n",
    "    split_point.append(len(dataset))\n",
    "\n",
    "    cba_rg_total_runtime = 0\n",
    "    cba_cb_total_runtime = 0\n",
    "    total_car_number = 0\n",
    "    total_classifier_rule_num = 0\n",
    "    error_total_rate = 0\n",
    "\n",
    "    for k in range(len(split_point)-1):\n",
    "        print(\"\\nRound %d:\" % k)\n",
    "\n",
    "        training_dataset = dataset[:split_point[k]] + dataset[split_point[k+1]:]\n",
    "        test_dataset = dataset[split_point[k]:split_point[k+1]]\n",
    "\n",
    "        start_time = time.time()\n",
    "        cars = rule_generator(training_dataset, minsup, minconf)\n",
    "        end_time = time.time()\n",
    "        cba_rg_runtime = end_time - start_time\n",
    "        cba_rg_total_runtime += cba_rg_runtime\n",
    "\n",
    "        start_time = time.time()\n",
    "        classifier_m1 = classifier_builder_m1(cars, training_dataset)\n",
    "        end_time = time.time()\n",
    "        cba_cb_runtime = end_time - start_time\n",
    "        cba_cb_total_runtime += cba_cb_runtime\n",
    "\n",
    "        error_rate = get_error_rate(classifier_m1, test_dataset)\n",
    "        error_total_rate += error_rate\n",
    "\n",
    "        total_car_number += len(cars.rules)\n",
    "        total_classifier_rule_num += len(classifier_m1.rule_list)\n",
    "\n",
    "        print(\"CBA's error rate without pruning: %.1lf%%\" % (error_rate * 100))\n",
    "        print(\"No. of CARs without pruning: %d\" % len(cars.rules))\n",
    "        print(\"CBA-RG's run time without pruning: %.2lf s\" % cba_rg_runtime)\n",
    "        print(\"CBA-CB M1's run time without pruning: %.2lf s\" % cba_cb_runtime)\n",
    "        print(\"No. of rules in classifier of CBA-CB M1 without pruning: %d\" % len(classifier_m1.rule_list))\n",
    "\n",
    "    print(\"\\nAverage CBA's error rate without pruning: %.1lf%%\" % (error_total_rate / 10 * 100))\n",
    "    print(\"Average No. of CARs without pruning: %d\" % int(total_car_number / 10))\n",
    "    print(\"Average CBA-RG's run time without pruning: %.2lf s\" % (cba_rg_total_runtime / 10))\n",
    "    print(\"Average CBA-CB M1's run time without pruning: %.2lf s\" % (cba_cb_total_runtime / 10))\n",
    "    print(\"Average No. of rules in classifier of CBA-CB M1 without pruning: %d\" % int(total_classifier_rule_num / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 0:\n",
      "CBA's error rate without pruning: 0.0%\n",
      "No. of CARs without pruning: 964\n",
      "CBA-RG's run time without pruning: 4.63 s\n",
      "CBA-CB M1's run time without pruning: 0.65 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 109\n",
      "\n",
      "Round 1:\n",
      "CBA's error rate without pruning: 0.0%\n",
      "No. of CARs without pruning: 454\n",
      "CBA-RG's run time without pruning: 1.07 s\n",
      "CBA-CB M1's run time without pruning: 0.06 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 59\n",
      "\n",
      "Round 2:\n",
      "CBA's error rate without pruning: 1.0%\n",
      "No. of CARs without pruning: 1514\n",
      "CBA-RG's run time without pruning: 3.85 s\n",
      "CBA-CB M1's run time without pruning: 0.50 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 129\n",
      "\n",
      "Round 3:\n",
      "CBA's error rate without pruning: 1.0%\n",
      "No. of CARs without pruning: 903\n",
      "CBA-RG's run time without pruning: 4.53 s\n",
      "CBA-CB M1's run time without pruning: 0.15 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 87\n",
      "\n",
      "Round 4:\n",
      "CBA's error rate without pruning: 0.0%\n",
      "No. of CARs without pruning: 1034\n",
      "CBA-RG's run time without pruning: 6.36 s\n",
      "CBA-CB M1's run time without pruning: 0.35 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 93\n",
      "\n",
      "Round 5:\n",
      "CBA's error rate without pruning: 0.0%\n",
      "No. of CARs without pruning: 1390\n",
      "CBA-RG's run time without pruning: 4.47 s\n",
      "CBA-CB M1's run time without pruning: 0.32 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 105\n",
      "\n",
      "Round 6:\n",
      "CBA's error rate without pruning: 0.0%\n",
      "No. of CARs without pruning: 757\n",
      "CBA-RG's run time without pruning: 8.99 s\n",
      "CBA-CB M1's run time without pruning: 0.13 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 80\n",
      "\n",
      "Round 7:\n",
      "CBA's error rate without pruning: 0.0%\n",
      "No. of CARs without pruning: 1315\n",
      "CBA-RG's run time without pruning: 9.72 s\n",
      "CBA-CB M1's run time without pruning: 0.28 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 94\n",
      "\n",
      "Round 8:\n",
      "CBA's error rate without pruning: 0.0%\n",
      "No. of CARs without pruning: 956\n",
      "CBA-RG's run time without pruning: 3.21 s\n",
      "CBA-CB M1's run time without pruning: 0.35 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 114\n",
      "\n",
      "Round 9:\n",
      "CBA's error rate without pruning: 0.0%\n",
      "No. of CARs without pruning: 754\n",
      "CBA-RG's run time without pruning: 5.72 s\n",
      "CBA-CB M1's run time without pruning: 0.14 s\n",
      "No. of rules in classifier of CBA-CB M1 without pruning: 72\n",
      "\n",
      "Average CBA's error rate without pruning: 0.2%\n",
      "Average No. of CARs without pruning: 1004\n",
      "Average CBA-RG's run time without pruning: 5.26 s\n",
      "Average CBA-CB M1's run time without pruning: 0.29 s\n",
      "Average No. of rules in classifier of CBA-CB M1 without pruning: 94\n"
     ]
    }
   ],
   "source": [
    "cross_validate_m1_without_prune(processed_data.values.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7af7df4901773a0e355da496bf365ae011b1b331a57bbc9908dae1ee21823d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
